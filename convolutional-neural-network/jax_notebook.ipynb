{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 02:50:16.110978: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-03 02:50:16.111058: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-03 02:50:16.111071: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "batch = jax.random.normal(rng, (4, 32, 32, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(batch_size=32):\n",
    "    \"\"\"\n",
    "    Creates train, validation, and test datasets.\n",
    "    Applies data normalization to all datasets and augmentation to training only.\n",
    "    \"\"\"\n",
    "    train_ds, val_ds, test_ds = tfds.load(\n",
    "        \"cifar10\", \n",
    "        split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
    "        as_supervised=True\n",
    "    )\n",
    "\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    var = [x ** 2 for x in std]\n",
    "\n",
    "    augment_pipeline = tf.keras.Sequential([\n",
    "        tf.keras.layers.Rescaling(scale=1./255),\n",
    "        tf.keras.layers.Normalization(mean=mean, variance=var),\n",
    "        tf.keras.layers.ZeroPadding2D(padding=(4, 4)),\n",
    "        tf.keras.layers.RandomFlip(mode=\"horizontal\"),\n",
    "        tf.keras.layers.RandomCrop(height=32, width=32)\n",
    "    ])\n",
    "\n",
    "    evaluate_pipeline = tf.keras.Sequential([\n",
    "        tf.keras.layers.Rescaling(scale=1./255),\n",
    "        tf.keras.layers.Normalization(mean=mean, variance=var),\n",
    "    ])\n",
    "\n",
    "    augment_pipeline.compile()\n",
    "    evaluate_pipeline.compile()\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    train_ds = train_ds.batch(batch_size, drop_remainder=True).map(lambda x, y: (augment_pipeline(x, training=True), y))\n",
    "    train_ds = train_ds.cache().shuffle(1000).prefetch(AUTOTUNE)\n",
    "\n",
    "    val_ds = val_ds.batch(batch_size, drop_remainder=True).map(lambda x, y: (evaluate_pipeline(x, training=False), y))\n",
    "    val_ds = val_ds.cache().prefetch(AUTOTUNE)\n",
    "    \n",
    "    test_ds = test_ds.batch(batch_size, drop_remainder=True).map(lambda x, y: (evaluate_pipeline(x, training=False), y))\n",
    "    test_ds = test_ds.cache().prefetch(AUTOTUNE)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "class IdentityResidual(nn.Module):\n",
    "    out_channels: int\n",
    "    stride: int = 1\n",
    "\n",
    "    def __call__(self, x):\n",
    "        _, _, _, c = x.shape  # BHWC\n",
    "        x = x[:, ::self.stride, ::self.stride, :]  # Downsample spatial dims\n",
    "        if c != self.out_channels:  # Pad extra channels\n",
    "            b, h, w, c = x.shape\n",
    "            pad = jnp.zeros((b, h, w, self.out_channels - c))\n",
    "            x = jnp.concatenate([x, pad], axis=-1)\n",
    "        return x\n",
    "\n",
    "class ResNetV2Layer(nn.Module):\n",
    "    out_channels: int\n",
    "    stride: int = 1\n",
    "\n",
    "    def setup(self):\n",
    "        conv_kwargs = {\"padding\": \"SAME\", \"use_bias\": False, \"kernel_size\": (3, 3)}\n",
    "        self.conv1 = nn.Conv(self.out_channels, strides=self.stride, **conv_kwargs)\n",
    "        self.conv2 = nn.Conv(self.out_channels, **conv_kwargs)\n",
    "        self.bn1 = nn.BatchNorm(use_running_average=True, momentum=0.9)  # Momentum set to match PyTorch\n",
    "        self.bn2 = nn.BatchNorm(use_running_average=True, momentum=0.9)\n",
    "        self.residual = IdentityResidual(self.out_channels, self.stride)\n",
    "        self.relu = nn.relu\n",
    "\n",
    "    def __call__(self, x):\n",
    "        residual = self.residual(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = ResNetV2Layer(32, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 16, 16, 32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables = layer.init(rng, batch)\n",
    "output = layer.apply(variables, batch)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "class ResNetV2Model(nn.Module):\n",
    "    output_classes: int = 10\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        return nn.Sequential([\n",
    "            nn.Conv(16, kernel_size=(3, 3), padding=\"SAME\", use_bias=False),\n",
    "            ResNetV2Layer(16),\n",
    "            ResNetV2Layer(16),\n",
    "            ResNetV2Layer(16),\n",
    "            ResNetV2Layer(32, stride=2),\n",
    "            ResNetV2Layer(32),\n",
    "            ResNetV2Layer(32),\n",
    "            ResNetV2Layer(64, stride=2),\n",
    "            ResNetV2Layer(64),\n",
    "            ResNetV2Layer(64),\n",
    "            partial(jnp.mean, axis=(1, 2)),  # Global average pooling over spatial dims\n",
    "            nn.Dense(self.output_classes)\n",
    "        ])(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNetV2Model()\n",
    "variables = model.init(rng, batch)\n",
    "output = model.apply(variables, batch)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, labels):\n",
    "    return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
    "\n",
    "def compute_metrics(*, logits, labels):\n",
    "    loss = loss_fn(logits, labels)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    metrics = {\n",
    "      'loss': loss,\n",
    "      'accuracy': accuracy,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    \"\"\"Custom train state for BatchNorm stats\"\"\"\n",
    "    batch_stats: Any\n",
    "\n",
    "def create_train_state(rng, learning_rate, batch_size, weight_decay=1e-4):\n",
    "    \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "    model = ResNetV2Model()\n",
    "    variables = model.init(rng, jnp.ones([batch_size, 32, 32, 3]))\n",
    "    params = variables[\"params\"]\n",
    "    batch_stats = variables[\"batch_stats\"]\n",
    "    tx = optax.adamw(learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    return TrainState.create(apply_fn=model.apply, params=params, batch_stats=batch_stats, tx=tx)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, inputs, labels):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    \n",
    "    def objective(params):\n",
    "        logits, updates = state.apply_fn(\n",
    "            {'params': params, 'batch_stats': state.batch_stats},\n",
    "            inputs, mutable=['batch_stats']  # Mutate batch stats during train step\n",
    "        )\n",
    "        loss = loss_fn(logits=logits, labels=labels)\n",
    "        return loss, (logits, updates)\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(objective, has_aux=True)\n",
    "    (loss, (logits, updates)), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    state = state.replace(batch_stats=updates['batch_stats'])  # Update with new batch stats\n",
    "    metrics = compute_metrics(logits=logits, labels=labels)\n",
    "    \n",
    "    return state, metrics\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(state, inputs, labels):\n",
    "    logits = state.apply_fn(\n",
    "        {'params': state.params, 'batch_stats': state.batch_stats},  # Use current batch stats in state\n",
    "        inputs  # Don't mutate batch stats in eval\n",
    "    )\n",
    "    metrics = compute_metrics(logits=logits, labels=labels)\n",
    "    return state, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(state, train_ds, epoch):\n",
    "    \"\"\"Train for a single epoch.\"\"\"\n",
    "    batch_metrics = []\n",
    "\n",
    "    for inputs, labels in train_ds:\n",
    "        inputs = jnp.float32(inputs)\n",
    "        labels = jnp.int32(labels)\n",
    "        \n",
    "        state, metrics = train_step(state, inputs, labels)\n",
    "        batch_metrics.append(metrics)\n",
    "\n",
    "    # compute mean of metrics across each batch in epoch.\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {\n",
    "        k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "        for k in batch_metrics_np[0]\n",
    "    }\n",
    "\n",
    "    print('train epoch: %d, loss: %.4f, accuracy: %.2f' % (epoch, epoch_metrics_np['loss'], epoch_metrics_np['accuracy'] * 100))\n",
    "\n",
    "    return state\n",
    "\n",
    "def eval_model(state, test_ds, epoch):\n",
    "    batch_metrics = []\n",
    "\n",
    "    for inputs, labels in test_ds:\n",
    "        inputs = jnp.float32(inputs)\n",
    "        labels = jnp.int32(labels)\n",
    "\n",
    "        state, metrics = eval_step(state, inputs, labels)\n",
    "        batch_metrics.append(metrics)\n",
    "    # compute mean of metrics across each batch in epoch.\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {\n",
    "        k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "        for k in batch_metrics_np[0]\n",
    "    }\n",
    "    print('eval epoch: %d, loss: %.4f, accuracy: %.2f' % (epoch, epoch_metrics_np['loss'], epoch_metrics_np['accuracy'] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformIntV2 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformIntV2 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformIntV2 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformIntV2 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "train_ds, eval_ds, test_ds = get_datasets(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, init_rng = jax.random.split(rng)\n",
    "LR = 1e-3\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "state = create_train_state(init_rng, LR, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1, loss: 1.7785, accuracy: 32.95\n",
      "eval epoch: 1, loss: 1.4341, accuracy: 46.43\n",
      "train epoch: 2, loss: 1.3733, accuracy: 49.56\n",
      "eval epoch: 2, loss: 1.3047, accuracy: 54.34\n",
      "train epoch: 3, loss: 1.1581, accuracy: 58.22\n",
      "eval epoch: 3, loss: 1.0763, accuracy: 62.18\n",
      "train epoch: 4, loss: 1.0057, accuracy: 63.97\n",
      "eval epoch: 4, loss: 0.9932, accuracy: 64.03\n",
      "train epoch: 5, loss: 0.8874, accuracy: 68.31\n",
      "eval epoch: 5, loss: 0.9396, accuracy: 66.89\n",
      "train epoch: 6, loss: 0.7865, accuracy: 71.81\n",
      "eval epoch: 6, loss: 0.9030, accuracy: 68.25\n",
      "train epoch: 7, loss: 0.6995, accuracy: 75.03\n",
      "eval epoch: 7, loss: 0.8431, accuracy: 71.85\n",
      "train epoch: 8, loss: 0.6120, accuracy: 78.25\n",
      "eval epoch: 8, loss: 0.8117, accuracy: 72.45\n",
      "train epoch: 9, loss: 0.5356, accuracy: 80.93\n",
      "eval epoch: 9, loss: 0.8818, accuracy: 72.23\n",
      "train epoch: 10, loss: 0.4742, accuracy: 83.04\n",
      "eval epoch: 10, loss: 0.9214, accuracy: 72.46\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # Use a separate PRNG key to permute image data during shuffling\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "    \n",
    "    # Run an optimization step over a training batch\n",
    "    state = train_epoch(state, train_ds, epoch)\n",
    "    \n",
    "    # Evaluate on the test set after each training epoch\n",
    "    eval_model(state, test_ds, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
