{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d744182d-f706-484b-bd7b-4027b39ae146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLinks: \\n\\nJax:\\nhttps://github.com/google/jax/tree/main/jax/example_libraries\\nhttps://teddykoker.com/2022/04/learning-to-learn-jax/\\nhttps://jax.readthedocs.io/en/latest/notebooks/neural_network_with_tfds_data.html\\nhttps://jax.readthedocs.io/en/latest/notebooks/convolutions.html\\nhttps://coderzcolumn.com/tutorials/artificial-intelligence/jax-guide-to-create-convolutional-neural-networks\\n\\nOptax:\\nhttps://github.com/deepmind/optax\\nhttps://optax.readthedocs.io/en/latest/optax-101.html\\n\\nFlax:\\nhttps://github.com/google/flax\\nhttps://flax.readthedocs.io/en/latest/getting_started.html\\nhttps://coderzcolumn.com/tutorials/artificial-intelligence/flax-cnn\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Links: \n",
    "\n",
    "Jax:\n",
    "https://github.com/google/jax/tree/main/jax/example_libraries\n",
    "https://teddykoker.com/2022/04/learning-to-learn-jax/\n",
    "https://jax.readthedocs.io/en/latest/notebooks/neural_network_with_tfds_data.html\n",
    "https://jax.readthedocs.io/en/latest/notebooks/convolutions.html\n",
    "https://coderzcolumn.com/tutorials/artificial-intelligence/jax-guide-to-create-convolutional-neural-networks\n",
    "\n",
    "Optax:\n",
    "https://github.com/deepmind/optax\n",
    "https://optax.readthedocs.io/en/latest/optax-101.html\n",
    "\n",
    "Flax:\n",
    "https://github.com/google/flax\n",
    "https://flax.readthedocs.io/en/latest/getting_started.html\n",
    "https://coderzcolumn.com/tutorials/artificial-intelligence/flax-cnn\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63791a54-9d48-4c95-8c4c-2b852dc2ff84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 18:39:33.515226: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-23 18:39:33.540994: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-23 18:39:34.009815: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-23 18:39:34.009869: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-23 18:39:34.009875: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/urban/anaconda3/envs/jax-flax-cpu/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Flax CNN Example using MNIST\n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp                # JAX NumPy\n",
    "\n",
    "from flax import linen as nn           # The Linen API\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "\n",
    "import numpy as np                     # Ordinary NumPy\n",
    "import optax                           # Optimizers\n",
    "import tensorflow_datasets as tfds     # TFDS for MNIST\n",
    "\n",
    "# Suppress warning and info messages from jax\n",
    "import os  \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5725e5ed-6ad7-408b-96fd-14f70cf4599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"A simple CNN model.\"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = x.reshape((x.shape[0], -1))  # flatten\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=10)(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "323e4bf5-07e4-4946-ac64-c0ea07833db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(*, logits, labels):\n",
    "    labels_onehot = jax.nn.one_hot(labels, num_classes=10)\n",
    "    return optax.softmax_cross_entropy(logits=logits, labels=labels_onehot).mean()\n",
    "\n",
    "def compute_metrics(*, logits, labels):\n",
    "    loss = cross_entropy_loss(logits=logits, labels=labels)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    metrics = {\n",
    "      'loss': loss,\n",
    "      'accuracy': accuracy,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def get_datasets():\n",
    "    \"\"\"Load MNIST train and test datasets into memory.\"\"\"\n",
    "    ds_builder = tfds.builder('mnist')\n",
    "    ds_builder.download_and_prepare()\n",
    "    train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n",
    "    test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n",
    "    train_ds['image'] = jnp.float32(train_ds['image']) / 255.\n",
    "    test_ds['image'] = jnp.float32(test_ds['image']) / 255.\n",
    "    \n",
    "    return train_ds, test_ds\n",
    "\n",
    "def create_train_state(rng, learning_rate, momentum):\n",
    "    \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "    cnn = CNN()\n",
    "    params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n",
    "    tx = optax.sgd(learning_rate, momentum)\n",
    "    \n",
    "    return train_state.TrainState.create(apply_fn=cnn.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "661ead9f-8328-429a-b6c1-9db8a3de8bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    \n",
    "    def loss_fn(params):\n",
    "        logits = CNN().apply({'params': params}, batch['image'])\n",
    "        loss = cross_entropy_loss(logits=logits, labels=batch['label'])\n",
    "        return loss, logits\n",
    "    \n",
    "    grad_fn = jax.grad(loss_fn, has_aux=True)\n",
    "    grads, logits = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(logits=logits, labels=batch['label'])\n",
    "    \n",
    "    return state, metrics\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(params, batch):\n",
    "    logits = CNN().apply({'params': params}, batch['image'])\n",
    "    return compute_metrics(logits=logits, labels=batch['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fe88c54-4148-4c5c-800d-8d18c1c93f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(state, train_ds, batch_size, epoch, rng):\n",
    "    \"\"\"Train for a single epoch.\"\"\"\n",
    "    train_ds_size = len(train_ds['image'])\n",
    "    steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "    perms = jax.random.permutation(rng, train_ds_size)\n",
    "    perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "    batch_metrics = []\n",
    "    \n",
    "    for perm in perms:\n",
    "        batch = {k: v[perm, ...] for k, v in train_ds.items()}\n",
    "        state, metrics = train_step(state, batch)\n",
    "        batch_metrics.append(metrics)\n",
    "\n",
    "    # compute mean of metrics across each batch in epoch.\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {\n",
    "      k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "      for k in batch_metrics_np[0]\n",
    "    }\n",
    "\n",
    "    print('train epoch: %d, loss: %.4f, accuracy: %.2f' % (epoch, epoch_metrics_np['loss'], epoch_metrics_np['accuracy'] * 100))\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87f4a906-99e7-4653-86ab-2b166215531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(params, test_ds):\n",
    "    metrics = eval_step(params, test_ds)\n",
    "    metrics = jax.device_get(metrics)\n",
    "    summary = jax.tree_util.tree_map(lambda x: x.item(), metrics)\n",
    "    return summary['loss'], summary['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eed1eb1-e186-4607-b45d-b295b065d68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 18:39:35.277165: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-23 18:39:35.277241: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-23 18:39:35.277288: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-23 18:39:35.277324: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-01-23 18:39:35.277360: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-01-23 18:39:35.277406: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-23 18:39:35.277446: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-23 18:39:35.277484: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-01-23 18:39:35.277493: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-01-23 18:39:36.097672: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 47040000 exceeds 10% of free system memory.\n",
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Suppress warning and info messages from jax\n",
    "import os  \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "train_ds, test_ds = get_datasets()\n",
    "print(train_ds['image'].shape)\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "\n",
    "state = create_train_state(init_rng, learning_rate, momentum)\n",
    "del init_rng  # Must not be used anymore.\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "765918d7-cead-4e21-85de-ab625364663e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1, loss: 0.1424, accuracy: 95.62\n",
      " test epoch: 1, loss: 0.06, accuracy: 98.24\n",
      "train epoch: 2, loss: 0.0485, accuracy: 98.51\n",
      " test epoch: 2, loss: 0.05, accuracy: 98.53\n",
      "train epoch: 3, loss: 0.0362, accuracy: 98.88\n",
      " test epoch: 3, loss: 0.03, accuracy: 99.10\n",
      "train epoch: 4, loss: 0.0253, accuracy: 99.21\n",
      " test epoch: 4, loss: 0.04, accuracy: 98.99\n",
      "train epoch: 5, loss: 0.0230, accuracy: 99.31\n",
      " test epoch: 5, loss: 0.03, accuracy: 99.15\n",
      "train epoch: 6, loss: 0.0169, accuracy: 99.50\n",
      " test epoch: 6, loss: 0.04, accuracy: 98.81\n",
      "train epoch: 7, loss: 0.0149, accuracy: 99.53\n",
      " test epoch: 7, loss: 0.03, accuracy: 99.09\n",
      "train epoch: 8, loss: 0.0130, accuracy: 99.59\n",
      " test epoch: 8, loss: 0.05, accuracy: 98.63\n",
      "train epoch: 9, loss: 0.0118, accuracy: 99.63\n",
      " test epoch: 9, loss: 0.05, accuracy: 98.93\n",
      "train epoch: 10, loss: 0.0101, accuracy: 99.69\n",
      " test epoch: 10, loss: 0.04, accuracy: 99.14\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Use a separate PRNG key to permute image data during shuffling\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "    # Run an optimization step over a training batch\n",
    "    state = train_epoch(state, train_ds, batch_size, epoch, input_rng)\n",
    "    # Evaluate on the test set after each training epoch\n",
    "    test_loss, test_accuracy = eval_model(state.params, test_ds)\n",
    "    print(' test epoch: %d, loss: %.2f, accuracy: %.2f' % (epoch, test_loss, test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99d91236-2920-4ae9-a436-048c03ccf3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    \"\"\"Load CIFAR10 train and test datasets into memory.\"\"\"\n",
    "    \n",
    "    ds_builder = tfds.builder('cifar10')\n",
    "    ds_builder.download_and_prepare()\n",
    "    train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n",
    "    test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n",
    "    train_ds['image'] = jnp.float32(train_ds['image']) / 255.\n",
    "    test_ds['image'] = jnp.float32(test_ds['image']) / 255.\n",
    "    \n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e10e9c0d-a0b0-421a-91c4-6fa6cfd11b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetV2(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # Input layer\n",
    "        x = nn.Conv(features=16, kernel_size=(3, 3), padding=1)(x)  # (4, 32, 32, 3) -> (4, 32, 32, 16)\n",
    "        x = nn.relu(x)\n",
    "        \n",
    "        # Block 1\n",
    "        x = nn.Conv(features=16, kernel_size=(3, 3), padding=1)(x)  # (4, 32, 32, 16) -> (4, 32, 32, 16)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=16, kernel_size=(3, 3), padding=1)(x)  # (4, 32, 32, 16) -> (4, 32, 32, 16)\n",
    "        x = nn.relu(x)\n",
    "        \n",
    "        # Block 2\n",
    "        x = nn.Conv(features=16, kernel_size=(3, 3), padding=1)(x)  # (4, 32, 32, 16) -> (4, 32, 32, 16)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=16, kernel_size=(3, 3), padding=1)(x)  # (4, 32, 32, 16) -> (4, 32, 32, 16)\n",
    "        x = nn.relu(x)\n",
    "        \n",
    "        # Block 3\n",
    "        x = nn.Conv(features=16, kernel_size=(3, 3), padding=1)(x)  # (4, 32, 32, 16) -> (4, 32, 32, 16)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=16, kernel_size=(3, 3), padding=1)(x)  # (4, 32, 32, 16) -> (4, 32, 32, 16)\n",
    "        x = nn.relu(x)\n",
    "        \n",
    "        # Block 4\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3), padding=1, strides=2)(x)  # (4, 32, 32, 16) -> (4, 16, 16, 32)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3), padding=1)(x)  # (4, 16, 16, 32) -> (4, 16, 16, 32)\n",
    "        x = nn.relu(x)\n",
    "        \n",
    "        # Block 5\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3), padding=1)(x)  # (4, 16, 16, 32) -> (4, 16, 16, 32)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3), padding=1)(x)  # (4, 16, 16, 32) -> (4, 16, 16, 32)\n",
    "        x = nn.relu(x)\n",
    "        \n",
    "        # Block 6\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3), padding=1)(x)  # (4, 16, 16, 32) -> (4, 16, 16, 32)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3), padding=1)(x)  # (4, 16, 16, 32) -> (4, 16, 16, 32)\n",
    "        x = nn.relu(x)\n",
    "        \n",
    "        # Block 7\n",
    "        x = nn.Conv(features=64, kernel_size=(3, 3), padding=1, strides=2)(x)  # (4, 16, 16, 32) -> (4, 8, 8, 64)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=64, kernel_size=(3, 3), padding=1)(x)  # (4, 8, 8, 64) -> (4, 8, 8, 64)\n",
    "        x = nn.relu(x)\n",
    "        \n",
    "        # Block 8\n",
    "        x = nn.Conv(features=64, kernel_size=(3, 3), padding=1)(x)  # (4, 8, 8, 64) -> (4, 8, 8, 64)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=64, kernel_size=(3, 3), padding=1)(x)  # (4, 8, 8, 64) -> (4, 8, 8, 64)\n",
    "        x = nn.relu(x)\n",
    "        \n",
    "        # Block 9\n",
    "        x = nn.Conv(features=64, kernel_size=(3, 3), padding=1)(x)  # (4, 8, 8, 64) -> (4, 8, 8, 64)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=64, kernel_size=(3, 3), padding=1)(x)  # (4, 8, 8, 64) -> (4, 8, 8, 64)\n",
    "        x = nn.relu(x)\n",
    "        \n",
    "        # Pooling \n",
    "        x = nn.avg_pool(x, window_shape=(8, 8)) # (4, 8, 8, 64) -> (4, 1, 1, 64)\n",
    "        x = x.flatten()  # flatten (4, 1, 1, 64) -> (4, 64)\n",
    "        \n",
    "        # Output \n",
    "        x = nn.Dense(features=10)(x)\n",
    "        x = nn.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd13813d-02e5-4957-9cfa-c2aab80c77ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, learning_rate, momentum):\n",
    "    \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "    cnn = ResNetV2()\n",
    "    params = cnn.init(rng, jnp.ones([1, 32, 32, 3]))['params']\n",
    "    tx = optax.sgd(learning_rate, momentum)\n",
    "    \n",
    "    return train_state.TrainState.create(apply_fn=cnn.apply, params=params, tx=tx)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    \n",
    "    def loss_fn(params):\n",
    "        logits = ResNetV2().apply({'params': params}, batch['image'])\n",
    "        loss = cross_entropy_loss(logits=logits, labels=batch['label'])\n",
    "        return loss, logits\n",
    "    \n",
    "    grad_fn = jax.grad(loss_fn, has_aux=True)\n",
    "    grads, logits = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(logits=logits, labels=batch['label'])\n",
    "    \n",
    "    return state, metrics\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(params, batch):\n",
    "    logits = ResNetV2().apply({'params': params}, batch['image'])\n",
    "    return compute_metrics(logits=logits, labels=batch['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80bbcd59-11bb-4cae-b0f3-5d9e3945f645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds = get_datasets()\n",
    "print(train_ds['image'].shape)\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "\n",
    "state = create_train_state(init_rng, learning_rate, momentum)\n",
    "del init_rng  # Must not be used anymore.\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15759f16-42ac-4a0a-bc7f-799d146e27fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "[b'train_00026' b'train_08926' b'train_36578' b'train_37193'\n b'train_25123' b'train_11103' b'train_00248' b'train_41776'\n b'train_05751' b'train_18224' b'train_20734' b'train_36163'\n b'train_37135' b'train_12608' b'train_49609' b'train_33290'\n b'train_27556' b'train_45969' b'train_14729' b'train_43136'\n b'train_27632' b'train_32697' b'train_37493' b'train_28656'\n b'train_23750' b'train_11580' b'train_04447' b'train_20043'\n b'train_13623' b'train_49115' b'train_49848' b'train_29417'\n b'train_21753' b'train_49643' b'train_39746' b'train_25872'\n b'train_16616' b'train_10649' b'train_24252' b'train_15527'\n b'train_30433' b'train_39122' b'train_12877' b'train_06903'\n b'train_36079' b'train_35962' b'train_05118' b'train_07392'\n b'train_16140' b'train_26435' b'train_47985' b'train_24918'\n b'train_36567' b'train_20746' b'train_46881' b'train_06261'\n b'train_11011' b'train_34093' b'train_23841' b'train_20323'\n b'train_45329' b'train_02794' b'train_46546' b'train_39896'\n b'train_33435' b'train_19078' b'train_27684' b'train_28542'\n b'train_31430' b'train_43474' b'train_20629' b'train_32453'\n b'train_20287' b'train_28801' b'train_16736' b'train_38079'\n b'train_34681' b'train_39609' b'train_29730' b'train_15414'\n b'train_39033' b'train_08409' b'train_37153' b'train_33968'\n b'train_49605' b'train_01034' b'train_18833' b'train_19936'\n b'train_46795' b'train_44068' b'train_22096' b'train_12910'\n b'train_23917' b'train_20540' b'train_26634' b'train_46994'\n b'train_24772' b'train_11476' b'train_01836' b'train_03829'\n b'train_08626' b'train_20234' b'train_12339' b'train_39595'\n b'train_01960' b'train_19396' b'train_20925' b'train_01635'\n b'train_10292' b'train_07263' b'train_41143' b'train_41793'\n b'train_05250' b'train_41491' b'train_34292' b'train_38887'\n b'train_11270' b'train_16006' b'train_39211' b'train_02668'\n b'train_43279' b'train_47732' b'train_46061' b'train_45053'\n b'train_15867' b'train_29419' b'train_01208' b'train_06841'\n b'train_07431' b'train_46674' b'train_17468' b'train_44065'\n b'train_44806' b'train_41527' b'train_04714' b'train_25564'\n b'train_35175' b'train_36564' b'train_44179' b'train_19423'\n b'train_24387' b'train_34771' b'train_48542' b'train_36400'\n b'train_08192' b'train_48607' b'train_44442' b'train_06062'\n b'train_14172' b'train_33054' b'train_05017' b'train_48154'\n b'train_04992' b'train_49040' b'train_44615' b'train_32510'\n b'train_46171' b'train_47596' b'train_42941' b'train_09260'\n b'train_17986' b'train_27938' b'train_26685' b'train_00584'\n b'train_07900' b'train_06967' b'train_13778' b'train_46311'\n b'train_08924' b'train_28844' b'train_39441' b'train_47543'\n b'train_47273' b'train_42377' b'train_08435' b'train_40420'\n b'train_41561' b'train_10408' b'train_11191' b'train_35551'\n b'train_45648' b'train_33154' b'train_02883' b'train_25050'\n b'train_00918' b'train_01412' b'train_48663' b'train_05266'\n b'train_46576' b'train_37752' b'train_08625' b'train_05078'\n b'train_00763' b'train_14982' b'train_15461' b'train_12765'\n b'train_07287' b'train_42926' b'train_22380' b'train_32854'\n b'train_38790' b'train_47926' b'train_33659' b'train_31426'\n b'train_09579' b'train_34882' b'train_09008' b'train_38034'\n b'train_14178' b'train_35073' b'train_00150' b'train_04191'\n b'train_49381' b'train_17084' b'train_13838' b'train_14745'\n b'train_05635' b'train_11864' b'train_47984' b'train_11367'\n b'train_11311' b'train_24270' b'train_22774' b'train_39602'\n b'train_09811' b'train_30465' b'train_20927' b'train_45931'\n b'train_13341' b'train_49500' b'train_22588' b'train_08604'\n b'train_03564' b'train_05421' b'train_36463' b'train_32404'\n b'train_11283' b'train_18021' b'train_45348' b'train_30212'\n b'train_44157' b'train_26257' b'train_17518' b'train_39870'\n b'train_23327' b'train_25393' b'train_39192' b'train_04489'\n b'train_36268' b'train_01280' b'train_14141' b'train_39401'\n b'train_44999' b'train_23205' b'train_11207' b'train_18613']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/jax-flax-cpu/lib/python3.8/site-packages/jax/interpreters/xla.py:273\u001b[0m, in \u001b[0;36mabstractify\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    272\u001b[0m aval_fn \u001b[38;5;241m=\u001b[39m pytype_aval_mappings\u001b[38;5;241m.\u001b[39mget(typ)\n\u001b[0;32m--> 273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aval_fn: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maval_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m typ \u001b[38;5;129;01min\u001b[39;00m typ\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__mro__\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/jax-flax-cpu/lib/python3.8/site-packages/jax/_src/abstract_arrays.py:36\u001b[0m, in \u001b[0;36mmake_shaped_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_shaped_array\u001b[39m(x):\n\u001b[0;32m---> 36\u001b[0m   dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     37\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m ShapedArray(np\u001b[38;5;241m.\u001b[39mshape(x), dtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/jax-flax-cpu/lib/python3.8/site-packages/jax/_src/dtypes.py:506\u001b[0m, in \u001b[0;36mresult_type\u001b[0;34m(return_weak_type_flag, *args)\u001b[0m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mat least one array or dtype is required\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 506\u001b[0m dtype, weak_type \u001b[38;5;241m=\u001b[39m \u001b[43m_lattice_result_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfloat_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weak_type:\n",
      "File \u001b[0;32m~/anaconda3/envs/jax-flax-cpu/lib/python3.8/site-packages/jax/_src/dtypes.py:465\u001b[0m, in \u001b[0;36m_lattice_result_type\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lattice_result_type\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[DType, \u001b[38;5;28mbool\u001b[39m]:\n\u001b[0;32m--> 465\u001b[0m   dtypes, weak_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_dtype_and_weaktype\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dtypes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/jax-flax-cpu/lib/python3.8/site-packages/jax/_src/dtypes.py:465\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lattice_result_type\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[DType, \u001b[38;5;28mbool\u001b[39m]:\n\u001b[0;32m--> 465\u001b[0m   dtypes, weak_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[43m_dtype_and_weaktype\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args))\n\u001b[1;32m    466\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dtypes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/jax-flax-cpu/lib/python3.8/site-packages/jax/_src/dtypes.py:313\u001b[0m, in \u001b[0;36m_dtype_and_weaktype\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03m\"\"\"Return a (dtype, weak_type) tuple for the given input.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28many\u001b[39m(value \u001b[38;5;129;01mis\u001b[39;00m typ \u001b[38;5;28;01mfor\u001b[39;00m typ \u001b[38;5;129;01min\u001b[39;00m _weak_types) \u001b[38;5;129;01mor\u001b[39;00m is_weakly_typed(value)\n",
      "File \u001b[0;32m~/anaconda3/envs/jax-flax-cpu/lib/python3.8/site-packages/jax/_src/dtypes.py:460\u001b[0m, in \u001b[0;36mdtype\u001b[0;34m(x, canonicalize)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _jax_dtype_set:\n\u001b[0;32m--> 460\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid JAX array \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    461\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype. Only arrays of numeric types are supported by JAX.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m canonicalize_dtype(dt) \u001b[38;5;28;01mif\u001b[39;00m canonicalize \u001b[38;5;28;01melse\u001b[39;00m dt\n",
      "\u001b[0;31mTypeError\u001b[0m: Value '[b'train_00026' b'train_08926' b'train_36578' b'train_37193'\n b'train_25123' b'train_11103' b'train_00248' b'train_41776'\n b'train_05751' b'train_18224' b'train_20734' b'train_36163'\n b'train_37135' b'train_12608' b'train_49609' b'train_33290'\n b'train_27556' b'train_45969' b'train_14729' b'train_43136'\n b'train_27632' b'train_32697' b'train_37493' b'train_28656'\n b'train_23750' b'train_11580' b'train_04447' b'train_20043'\n b'train_13623' b'train_49115' b'train_49848' b'train_29417'\n b'train_21753' b'train_49643' b'train_39746' b'train_25872'\n b'train_16616' b'train_10649' b'train_24252' b'train_15527'\n b'train_30433' b'train_39122' b'train_12877' b'train_06903'\n b'train_36079' b'train_35962' b'train_05118' b'train_07392'\n b'train_16140' b'train_26435' b'train_47985' b'train_24918'\n b'train_36567' b'train_20746' b'train_46881' b'train_06261'\n b'train_11011' b'train_34093' b'train_23841' b'train_20323'\n b'train_45329' b'train_02794' b'train_46546' b'train_39896'\n b'train_33435' b'train_19078' b'train_27684' b'train_28542'\n b'train_31430' b'train_43474' b'train_20629' b'train_32453'\n b'train_20287' b'train_28801' b'train_16736' b'train_38079'\n b'train_34681' b'train_39609' b'train_29730' b'train_15414'\n b'train_39033' b'train_08409' b'train_37153' b'train_33968'\n b'train_49605' b'train_01034' b'train_18833' b'train_19936'\n b'train_46795' b'train_44068' b'train_22096' b'train_12910'\n b'train_23917' b'train_20540' b'train_26634' b'train_46994'\n b'train_24772' b'train_11476' b'train_01836' b'train_03829'\n b'train_08626' b'train_20234' b'train_12339' b'train_39595'\n b'train_01960' b'train_19396' b'train_20925' b'train_01635'\n b'train_10292' b'train_07263' b'train_41143' b'train_41793'\n b'train_05250' b'train_41491' b'train_34292' b'train_38887'\n b'train_11270' b'train_16006' b'train_39211' b'train_02668'\n b'train_43279' b'train_47732' b'train_46061' b'train_45053'\n b'train_15867' b'train_29419' b'train_01208' b'train_06841'\n b'train_07431' b'train_46674' b'train_17468' b'train_44065'\n b'train_44806' b'train_41527' b'train_04714' b'train_25564'\n b'train_35175' b'train_36564' b'train_44179' b'train_19423'\n b'train_24387' b'train_34771' b'train_48542' b'train_36400'\n b'train_08192' b'train_48607' b'train_44442' b'train_06062'\n b'train_14172' b'train_33054' b'train_05017' b'train_48154'\n b'train_04992' b'train_49040' b'train_44615' b'train_32510'\n b'train_46171' b'train_47596' b'train_42941' b'train_09260'\n b'train_17986' b'train_27938' b'train_26685' b'train_00584'\n b'train_07900' b'train_06967' b'train_13778' b'train_46311'\n b'train_08924' b'train_28844' b'train_39441' b'train_47543'\n b'train_47273' b'train_42377' b'train_08435' b'train_40420'\n b'train_41561' b'train_10408' b'train_11191' b'train_35551'\n b'train_45648' b'train_33154' b'train_02883' b'train_25050'\n b'train_00918' b'train_01412' b'train_48663' b'train_05266'\n b'train_46576' b'train_37752' b'train_08625' b'train_05078'\n b'train_00763' b'train_14982' b'train_15461' b'train_12765'\n b'train_07287' b'train_42926' b'train_22380' b'train_32854'\n b'train_38790' b'train_47926' b'train_33659' b'train_31426'\n b'train_09579' b'train_34882' b'train_09008' b'train_38034'\n b'train_14178' b'train_35073' b'train_00150' b'train_04191'\n b'train_49381' b'train_17084' b'train_13838' b'train_14745'\n b'train_05635' b'train_11864' b'train_47984' b'train_11367'\n b'train_11311' b'train_24270' b'train_22774' b'train_39602'\n b'train_09811' b'train_30465' b'train_20927' b'train_45931'\n b'train_13341' b'train_49500' b'train_22588' b'train_08604'\n b'train_03564' b'train_05421' b'train_36463' b'train_32404'\n b'train_11283' b'train_18021' b'train_45348' b'train_30212'\n b'train_44157' b'train_26257' b'train_17518' b'train_39870'\n b'train_23327' b'train_25393' b'train_39192' b'train_04489'\n b'train_36268' b'train_01280' b'train_14141' b'train_39401'\n b'train_44999' b'train_23205' b'train_11207' b'train_18613']' with dtype object is not a valid JAX array type. Only arrays of numeric types are supported by JAX.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [33], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m rng, input_rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(rng)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Run an optimization step over a training batch\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_rng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Evaluate on the test set after each training epoch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m eval_model(state\u001b[38;5;241m.\u001b[39mparams, test_ds)\n",
      "Cell \u001b[0;32mIn [8], line 13\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(state, train_ds, batch_size, epoch, rng)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m perm \u001b[38;5;129;01min\u001b[39;00m perms:\n\u001b[1;32m     12\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v[perm, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m train_ds\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 13\u001b[0m     state, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     batch_metrics\u001b[38;5;241m.\u001b[39mappend(metrics)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# compute mean of metrics across each batch in epoch.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/jax-flax-cpu/lib/python3.8/site-packages/jax/core.py:1447\u001b[0m, in \u001b[0;36mConcreteArray.__init__\u001b[0;34m(self, dtype, val, weak_type)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(np\u001b[38;5;241m.\u001b[39mresult_type(val)), (val, dtype)\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval \u001b[38;5;241m=\u001b[39m val\n\u001b[0;32m-> 1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m), val\n",
      "\u001b[0;31mAssertionError\u001b[0m: [b'train_00026' b'train_08926' b'train_36578' b'train_37193'\n b'train_25123' b'train_11103' b'train_00248' b'train_41776'\n b'train_05751' b'train_18224' b'train_20734' b'train_36163'\n b'train_37135' b'train_12608' b'train_49609' b'train_33290'\n b'train_27556' b'train_45969' b'train_14729' b'train_43136'\n b'train_27632' b'train_32697' b'train_37493' b'train_28656'\n b'train_23750' b'train_11580' b'train_04447' b'train_20043'\n b'train_13623' b'train_49115' b'train_49848' b'train_29417'\n b'train_21753' b'train_49643' b'train_39746' b'train_25872'\n b'train_16616' b'train_10649' b'train_24252' b'train_15527'\n b'train_30433' b'train_39122' b'train_12877' b'train_06903'\n b'train_36079' b'train_35962' b'train_05118' b'train_07392'\n b'train_16140' b'train_26435' b'train_47985' b'train_24918'\n b'train_36567' b'train_20746' b'train_46881' b'train_06261'\n b'train_11011' b'train_34093' b'train_23841' b'train_20323'\n b'train_45329' b'train_02794' b'train_46546' b'train_39896'\n b'train_33435' b'train_19078' b'train_27684' b'train_28542'\n b'train_31430' b'train_43474' b'train_20629' b'train_32453'\n b'train_20287' b'train_28801' b'train_16736' b'train_38079'\n b'train_34681' b'train_39609' b'train_29730' b'train_15414'\n b'train_39033' b'train_08409' b'train_37153' b'train_33968'\n b'train_49605' b'train_01034' b'train_18833' b'train_19936'\n b'train_46795' b'train_44068' b'train_22096' b'train_12910'\n b'train_23917' b'train_20540' b'train_26634' b'train_46994'\n b'train_24772' b'train_11476' b'train_01836' b'train_03829'\n b'train_08626' b'train_20234' b'train_12339' b'train_39595'\n b'train_01960' b'train_19396' b'train_20925' b'train_01635'\n b'train_10292' b'train_07263' b'train_41143' b'train_41793'\n b'train_05250' b'train_41491' b'train_34292' b'train_38887'\n b'train_11270' b'train_16006' b'train_39211' b'train_02668'\n b'train_43279' b'train_47732' b'train_46061' b'train_45053'\n b'train_15867' b'train_29419' b'train_01208' b'train_06841'\n b'train_07431' b'train_46674' b'train_17468' b'train_44065'\n b'train_44806' b'train_41527' b'train_04714' b'train_25564'\n b'train_35175' b'train_36564' b'train_44179' b'train_19423'\n b'train_24387' b'train_34771' b'train_48542' b'train_36400'\n b'train_08192' b'train_48607' b'train_44442' b'train_06062'\n b'train_14172' b'train_33054' b'train_05017' b'train_48154'\n b'train_04992' b'train_49040' b'train_44615' b'train_32510'\n b'train_46171' b'train_47596' b'train_42941' b'train_09260'\n b'train_17986' b'train_27938' b'train_26685' b'train_00584'\n b'train_07900' b'train_06967' b'train_13778' b'train_46311'\n b'train_08924' b'train_28844' b'train_39441' b'train_47543'\n b'train_47273' b'train_42377' b'train_08435' b'train_40420'\n b'train_41561' b'train_10408' b'train_11191' b'train_35551'\n b'train_45648' b'train_33154' b'train_02883' b'train_25050'\n b'train_00918' b'train_01412' b'train_48663' b'train_05266'\n b'train_46576' b'train_37752' b'train_08625' b'train_05078'\n b'train_00763' b'train_14982' b'train_15461' b'train_12765'\n b'train_07287' b'train_42926' b'train_22380' b'train_32854'\n b'train_38790' b'train_47926' b'train_33659' b'train_31426'\n b'train_09579' b'train_34882' b'train_09008' b'train_38034'\n b'train_14178' b'train_35073' b'train_00150' b'train_04191'\n b'train_49381' b'train_17084' b'train_13838' b'train_14745'\n b'train_05635' b'train_11864' b'train_47984' b'train_11367'\n b'train_11311' b'train_24270' b'train_22774' b'train_39602'\n b'train_09811' b'train_30465' b'train_20927' b'train_45931'\n b'train_13341' b'train_49500' b'train_22588' b'train_08604'\n b'train_03564' b'train_05421' b'train_36463' b'train_32404'\n b'train_11283' b'train_18021' b'train_45348' b'train_30212'\n b'train_44157' b'train_26257' b'train_17518' b'train_39870'\n b'train_23327' b'train_25393' b'train_39192' b'train_04489'\n b'train_36268' b'train_01280' b'train_14141' b'train_39401'\n b'train_44999' b'train_23205' b'train_11207' b'train_18613']"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    \n",
    "    # Use a separate PRNG key to permute image data during shuffling\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "    \n",
    "    # Run an optimization step over a training batch\n",
    "    state = train_epoch(state, train_ds, batch_size, epoch, input_rng)\n",
    "    \n",
    "    # Evaluate on the test set after each training epoch\n",
    "    test_loss, test_accuracy = eval_model(state.params, test_ds)\n",
    "    \n",
    "    print(' test epoch: %d, loss: %.2f, accuracy: %.2f' % (epoch, test_loss, test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683d8a47-dfad-4c79-be13-ebdcd6fed784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-flax-cpu",
   "language": "python",
   "name": "jax-flax-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
