{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "717962b8",
   "metadata": {},
   "source": [
    "# Milestone 2: Covolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f8b80",
   "metadata": {},
   "source": [
    "Making a ResNetV2-20 model to perform the CIFAR-10 image classification task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc567535",
   "metadata": {},
   "source": [
    "## Model Specfications"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1ce7939",
   "metadata": {},
   "source": [
    "Model: ResNetV2-20\n",
    "- Input layer: Input size: (32 x 32) x 3\n",
    "    - conv2d (3 x 3) x 64\n",
    "- ResBlock 1: Input size (32 x 32) x 64\n",
    "     - conv2d (3 x 3) x 16\n",
    "     - conv2d (3 x 3) x 16\n",
    "- ResBlock 2: Input size (32 x 32) x 16\n",
    "     - conv2d (3 x 3) x 16\n",
    "     - conv2d (3 x 3) x 16\n",
    "- ResBlock 3: Input size (32 x 32) x 16\n",
    "     - conv2d (3 x 3) x 16\n",
    "     - conv2d (3 x 3) x 16  \n",
    "- ResBlock 4: Input size (32 x 32) x 16\n",
    "     - conv2d (3 x 3) x 32, stride 2\n",
    "     - conv2d (3 x 3) x 32\n",
    "- ResBlock 5: Input size (16 x 16) x 32\n",
    "     - conv2d (3 x 3) x 32\n",
    "     - conv2d (3 x 3) x 32\n",
    "- ResBlock 6: Input size (16 x 16) x 32\n",
    "     - conv2d (3 x 3) x 32\n",
    "     - conv2d (3 x 3) x 32\n",
    "- ResBlock 7: Input size (16 x 16) x 32\n",
    "     - conv2d (3 x 3) x 64, stride 2\n",
    "     - conv2d (3 x 3) x 64\n",
    "- ResBlock 8: Input size (8 x 8) x 64\n",
    "     - conv2d (3 x 3) x 64\n",
    "     - conv2d (3 x 3) x 64\n",
    "- ResBlock 9: Input size (8 x 8) x 64\n",
    "     - conv2d (3 x 3) x 64\n",
    "     - conv2d (3 x 3) x 64\n",
    "- Pooling: input size (8 x 8) x 64\n",
    "     - GlobalAveragePooling/AdaptiveAveragePooling((1,1))\n",
    "- Output layer: Input size (64,)\n",
    "     - Dense/Linear (64,10)\n",
    "     - Activation: Softmax\n",
    "\n",
    "\n",
    "\n",
    "Data: CIFAR-10 tiny images\n",
    "- 32 x 32 x 3 RGB colour images\n",
    "- Train/Test split: Use data splits already given (50,000 train, 10,000 test). From the 50,000 train images, use 45,000 for training and 5,000 for validation every epoch inside the training loop. Reserve the 10,000 test set images for final evaluation.\n",
    "- Pre-processing inputs: \n",
    "     - Depending on data source, scale int8 inputs to [0, 1] by dividing by 255\n",
    "     - ImageNet normalization \n",
    "          - From the RGB channels, subtract means [0.485, 0.456, 0.406] and divide by standard deviations [0.229, 0.224, 0.225]\n",
    "     - 4 pixel padding on each side (36x36), then apply 32x32 crop randomly sampled from the padded image or its horizontal flip as in Section 3.2 of [3]\n",
    "- Preprocessing labels: Use integer indices\n",
    "\n",
    "\n",
    "Hyperparameters:\n",
    "- Optimizer: AdamW\n",
    "- learning rate: 1e-3 \n",
    "- beta_1: 0.9\n",
    "- beta_2: 0.999\n",
    "- weight decay: 0.0001\n",
    "- Number of epochs for training: 50 (TBD)\n",
    "- Batch size: 256 (TBD)\n",
    "\n",
    "\n",
    "Metrics to record:\n",
    "- Total training time (from start of training script to end of training run)\n",
    "- Training time per 1 epoch (measure from start to end of each epoch and average over all epochs)\n",
    "- Inference time per batch (measure per batch and average over all batches)\n",
    "- Last epoch training loss\n",
    "- Last epoch eval accuracy (from the 5,000 evaluation dataset)\n",
    "- Held-out test set accuracy (from the 10,000 test dataset)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b51afea",
   "metadata": {},
   "source": [
    "<h2> Library import </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gluoncv/__init__.py:40: UserWarning: Both `mxnet==1.9.1` and `torch==1.13.1+cu117` are installed. You might encounter increased GPU memory footprint if both framework are used at the same time.\n",
      "  warnings.warn(f'Both `mxnet=={mx.__version__}` and `torch=={torch.__version__}` are installed. '\n"
     ]
    }
   ],
   "source": [
    "# Necessary Libraries\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, autograd as ag, npx\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "\n",
    "# Libraries for datasets and pre-preprocessing\n",
    "from mxnet.gluon.data.vision import transforms, CIFAR10\n",
    "import gluoncv\n",
    "from gluoncv.data import transforms as gcv_transforms\n",
    "import torch.utils # needed to split the training DS into train_data and cv_data\n",
    "\n",
    "\n",
    "# json library neded to export metrics \n",
    "import json\n",
    "import time\n",
    "\n",
    "# Miscellaneous libraries incase I need them for testing\n",
    "import matplotlib as plt\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT if multiple gpus\n",
    "# # number of GPUs to use\n",
    "num_gpus = 1\n",
    "ctx = [mx.gpu(i) for i in range(num_gpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels just for reference\n",
    "labels = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\"\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Dataset import & Data pre-processing/transformation </h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Transformation functions </h3>\n",
    "\n",
    "<p> transform_train will be used on both train_data and cv_data, while transform_test will be used on test_data. Since training dataset provides more randomized data (and should be more generalizable), I will not be performing the random operations on the testing dataset. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([ gcv_transforms.RandomCrop(32, pad=4), # Randomly crop an area and resize it to be 32x32, then pad it to be 40x40 \n",
    "                                    transforms.RandomFlipLeftRight(), # Applying a random horizontal flip\n",
    "                                    transforms.ToTensor(), # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "                                                           # and map values from [0, 255] to [0,1]\n",
    "                                    # Normalize the image with mean and standard deviation calculated across all images\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "                                ])\n",
    "\n",
    "transform_test = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "                                ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Importing transformed datasets and splitting train and cv </h3>\n",
    "\n",
    "<h4> IMPORTANT: Run the Following cells if you want the full dataset (50,000 train + 10,000 test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ./data/cifar-10-binary.tar.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/cifar10/cifar-10-binary.tar.gz...\n"
     ]
    }
   ],
   "source": [
    "# Creating the train and test DS\n",
    "full_train_ds = CIFAR10(train=True, root=\"./data\").transform_first(transform_train, lazy=True)\n",
    "test_ds = CIFAR10(train= False, root=\"./data\").transform_first(transform_test, lazy=True)\n",
    "\n",
    "# Splitting the training datasets into the train_data and cv_data\n",
    "train_size = int(0.9 * len(full_train_ds))\n",
    "cv_size = len(full_train_ds) - train_size\n",
    "train_ds, cv_ds = torch.utils.data.random_split(full_train_ds, [train_size, cv_size]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset\t\t Length \t\t Type\n",
      "full_train_ds \t 50000 \t <class 'mxnet.gluon.data.dataset.SimpleDataset'>\n",
      "train_ds \t 45000 \t <class 'torch.utils.data.dataset.Subset'>\n",
      "cv_ds \t\t 5000 \t <class 'torch.utils.data.dataset.Subset'>\n",
      "test_ds \t 10000 \t <class 'mxnet.gluon.data.dataset.SimpleDataset'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset\\t\\t Length \\t\\t Type\")\n",
    "print(\"full_train_ds\",\"\\t\", len(full_train_ds), \"\\t\", type(full_train_ds))\n",
    "print(\"train_ds\",\"\\t\", len(train_ds), \"\\t\", type(train_ds))\n",
    "print(\"cv_ds\",\"\\t\\t\", len(cv_ds), \"\\t\", type(cv_ds))\n",
    "print(\"test_ds\",\"\\t\", len(test_ds), \"\\t\", type(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets into the DataLoader\n",
    "batch_size = 128\n",
    "train_data = gluon.data.DataLoader(train_ds, batch_size=batch_size,  shuffle=True, last_batch='discard')\n",
    "cv_data = gluon.data.DataLoader(cv_ds, batch_size=batch_size,  shuffle=False, last_batch='discard')\n",
    "test_data = gluon.data.DataLoader(test_ds, batch_size=batch_size,  shuffle=False, last_batch='discard')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(351, 39, 78)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(cv_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data\t (256, 3, 32, 32) (256,)\n",
      "cv_data\t\t (256, 3, 32, 32) (256,)\n",
      "test_data\t (256, 3, 32, 32) (256,)\n"
     ]
    }
   ],
   "source": [
    "# see data shape\n",
    "for data, label in train_data:\n",
    "    print(\"train_data\\t\", data.shape, label.shape)\n",
    "    break\n",
    "for data, label in cv_data:\n",
    "    print(\"cv_data\\t\\t\",data.shape, label.shape)\n",
    "    break\n",
    "for data, label in test_data:\n",
    "    print(\"test_data\\t\",data.shape, label.shape)\n",
    "    break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> IMPORTANT: Run the following cells below if you want to use the proof-of-concept dataset (1024 train + 10000 test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the train and test DS\n",
    "full_train_ds = CIFAR10(train=True).transform_first(transform_train, lazy=False)\n",
    "test_ds = CIFAR10(train= False).transform_first(transform_test, lazy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF USING PROOF-OF-CONCEPT\n",
    "poc_ds = full_train_ds[0:1024]\n",
    "\n",
    "train_size = int(0.9 * len(poc_ds))\n",
    "cv_size = len(poc_ds) - train_size\n",
    "train_ds, cv_ds = torch.utils.data.random_split(poc_ds, [train_size, cv_size]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data\t (3, 32, 32) ()\n",
      "train_ds + cv_ds = poc_ds\n",
      "921 \t + 103 \t = 1024\n"
     ]
    }
   ],
   "source": [
    "for data, label in poc_ds:\n",
    "    print(\"train_data\\t\", data.shape, label.shape)\n",
    "    break\n",
    "print(\"train_ds + cv_ds = poc_ds\")\n",
    "print(len(train_ds), \"\\t +\", len(cv_ds), \"\\t =\", len(poc_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets into the DataLoader\n",
    "batch_size = 64\n",
    "train_data = gluon.data.DataLoader(train_ds, batch_size = batch_size , shuffle=True, last_batch= 'discard')\n",
    "cv_data = gluon.data.DataLoader(cv_ds, batch_size = batch_size,  shuffle=True , last_batch= 'keep')\n",
    "test_data = gluon.data.DataLoader(test_ds, batch_size= batch_size,  shuffle=True , last_batch= 'keep')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data\t (64, 3, 32, 32) (64,)\n",
      "cv_data\t\t (64, 3, 32, 32) (64,)\n",
      "test_data\t (64, 3, 32, 32) (64,)\n",
      "14\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# see data shape\n",
    "for data, label in train_data:\n",
    "    print(\"train_data\\t\", data.shape, label.shape)\n",
    "    break\n",
    "for data, label in cv_data:\n",
    "    print(\"cv_data\\t\\t\",data.shape, label.shape)\n",
    "    break\n",
    "for data, label in test_data:\n",
    "    print(\"test_data\\t\",data.shape, label.shape)\n",
    "    break\n",
    "print(len(train_data))\n",
    "print(len(cv_data)) # total is 16 batch steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Defining ResNetV2 class structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Defining the Basic Block structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Block):\n",
    "    def __init__ (self, in_channels, channels, strides = 1 , **kwargs):\n",
    "        super(BasicBlock, self).__init__(**kwargs)\n",
    "        conv_kwargs = {\n",
    "            \"kernel_size\": (3,3),\n",
    "            \"padding\": 1,\n",
    "            \"use_bias\": False\n",
    "        }\n",
    "        self.strides = strides\n",
    "        self.in_channels = in_channels\n",
    "        self.channels = channels\n",
    "\n",
    "        self.bn1 = nn.BatchNorm(in_channels= in_channels)        \n",
    "        self.conv1 = nn.Conv2D(channels, strides= strides,  in_channels= in_channels, **conv_kwargs) \n",
    "        \n",
    "        self.bn2 = nn.BatchNorm(in_channels= channels)\n",
    "        self.conv2 = nn.Conv2D(channels, in_channels= channels, **conv_kwargs)\n",
    "        self.relu = nn.Activation('relu')\n",
    "        \n",
    "    def downsample(self,x):\n",
    "    # Downsample with 'nearest' method (this is striding if dims are divisible by stride)\n",
    "    # Equivalently x = x[:, :, ::stride, ::stride].contiguous()   \n",
    "        x = x[:,:, ::self.strides, ::self.strides]\n",
    "        #creating padding tenspr for extra channels\n",
    "        (b, c, h, w) = x.shape\n",
    "        num_pad_channels = self.channels - self.in_channels\n",
    "        pad = mx.nd.zeros((b, num_pad_channels, h,w))\n",
    "        # append this padding to the downsampled identity\n",
    "        x = mx.nd.concat(x , pad, dim = 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.strides > 1:\n",
    "            residual = self.downsample(x)\n",
    "        else:\n",
    "            residual = x\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        return x + residual"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Defining the ResNetV2 CNN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetV2(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ResNetV2, self).__init__(**kwargs)\n",
    "\n",
    "        self.input_layer = nn.Conv2D(in_channels = 3, channels= 16, kernel_size=(3,3), padding=1)\n",
    "\n",
    "        self.layer_1 = BasicBlock(16,16)\n",
    "        self.layer_2 = BasicBlock(16,16)\n",
    "        self.layer_3 = BasicBlock(16,16)\n",
    "\n",
    "        self.layer_4 = BasicBlock(16,32, strides = 2)\n",
    "        self.layer_5 = BasicBlock(32,32)\n",
    "        self.layer_6 = BasicBlock(32,32)\n",
    "\n",
    "        self.layer_7 = BasicBlock(32,64, strides = 2)\n",
    "        self.layer_8 = BasicBlock(64,64)\n",
    "        self.layer_9 = BasicBlock(64,64)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.pool = nn.GlobalAvgPool2D(layout = 'NCHW')\n",
    "        self.output_layer = nn.Dense(units=10, in_units=64)\n",
    "\n",
    "    \n",
    "    def forward (self, x):\n",
    "        out = self.input_layer(x)\n",
    "        out = self.layer_1(out)\n",
    "        out = self.layer_2(out)\n",
    "        out = self.layer_3(out)\n",
    "        out = self.layer_4(out)\n",
    "        out = self.layer_5(out)\n",
    "        out = self.layer_6(out)\n",
    "        out = self.layer_7(out)\n",
    "        out = self.layer_8(out)\n",
    "        out = self.layer_9(out)\n",
    "        # print(\"Before Pool: \", out.shape)\n",
    "        out = self.pool(out)\n",
    "        # print(\"After Pool: \", out.shape)\n",
    "        out = self.flatten(out)\n",
    "        # print(\"After Flattening: \", out.shape)\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNetV2()\n",
    "net.initialize()\n",
    "# net.collect_params\n",
    "# print(net.collect_params)\n",
    "# net.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sanity check to see all the layers\n",
    "# params = net.collect_params()\n",
    "\n",
    "# for key, value in params.items():\n",
    "#     print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(params = net.collect_params(),\n",
    "                    optimizer='adam',\n",
    "                    optimizer_params = {'learning_rate': 0.001, 'beta1': 0.9, 'beta2': 0.999, 'wd':0.0001}\n",
    "                    ) # The guidelines state using AdamW optimizer, unsure whether 'adam' is sufficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # second sanity check to see whether running a rand input results in no errors\n",
    "# inputs = mx.np.random.normal(size=(4, 3, 32, 32)).as_nd_ndarray()\n",
    "# outputs = net(inputs)\n",
    "# outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Running model on Training and CV dataset  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 2.159735, Train_acc: 0.157366, Val_acc: 0.106796, in 3.14s \n",
      "Epoch 2 | Loss: 1.895579, Train_acc: 0.241071, Val_acc: 0.203883, in 3.10s \n",
      "Epoch 3 | Loss: 1.769139, Train_acc: 0.311384, Val_acc: 0.300971, in 2.64s \n",
      "Epoch 4 | Loss: 1.726316, Train_acc: 0.314732, Val_acc: 0.262136, in 2.50s \n",
      "Epoch 5 | Loss: 1.624458, Train_acc: 0.360491, Val_acc: 0.203883, in 2.53s \n",
      "Epoch 6 | Loss: 1.529403, Train_acc: 0.404018, Val_acc: 0.320388, in 2.55s \n",
      "Epoch 7 | Loss: 1.474895, Train_acc: 0.430804, Val_acc: 0.378641, in 2.48s \n",
      "Epoch 8 | Loss: 1.385179, Train_acc: 0.463170, Val_acc: 0.368932, in 2.83s \n",
      "Epoch 9 | Loss: 1.289832, Train_acc: 0.500000, Val_acc: 0.359223, in 2.53s \n",
      "Epoch 10 | Loss: 1.277779, Train_acc: 0.504464, Val_acc: 0.174757, in 2.69s \n",
      "Epoch 11 | Loss: 1.140953, Train_acc: 0.553571, Val_acc: 0.339806, in 2.64s \n",
      "Epoch 12 | Loss: 1.038478, Train_acc: 0.593750, Val_acc: 0.446602, in 2.71s \n",
      "Epoch 13 | Loss: 0.989179, Train_acc: 0.651786, Val_acc: 0.262136, in 2.85s \n",
      "Epoch 14 | Loss: 0.920590, Train_acc: 0.647321, Val_acc: 0.359223, in 2.52s \n",
      "Epoch 15 | Loss: 0.757903, Train_acc: 0.710938, Val_acc: 0.417476, in 2.62s \n",
      "Epoch 16 | Loss: 0.627575, Train_acc: 0.772321, Val_acc: 0.388350, in 3.01s \n",
      "Epoch 17 | Loss: 0.494563, Train_acc: 0.832589, Val_acc: 0.398058, in 2.71s \n",
      "Epoch 18 | Loss: 0.413724, Train_acc: 0.856027, Val_acc: 0.407767, in 2.82s \n",
      "Epoch 19 | Loss: 0.429941, Train_acc: 0.852679, Val_acc: 0.330097, in 2.82s \n",
      "Epoch 20 | Loss: 0.311896, Train_acc: 0.886161, Val_acc: 0.349515, in 2.51s \n",
      "Epoch 21 | Loss: 0.334266, Train_acc: 0.881696, Val_acc: 0.320388, in 2.43s \n",
      "Epoch 22 | Loss: 0.298359, Train_acc: 0.895089, Val_acc: 0.446602, in 2.47s \n",
      "Epoch 23 | Loss: 0.239002, Train_acc: 0.927455, Val_acc: 0.339806, in 2.41s \n",
      "Epoch 24 | Loss: 0.184371, Train_acc: 0.940848, Val_acc: 0.339806, in 2.49s \n",
      "Epoch 25 | Loss: 0.159670, Train_acc: 0.940848, Val_acc: 0.378641, in 2.38s \n",
      "Epoch 26 | Loss: 0.146207, Train_acc: 0.950893, Val_acc: 0.398058, in 2.49s \n",
      "Epoch 27 | Loss: 0.141667, Train_acc: 0.949777, Val_acc: 0.330097, in 2.38s \n",
      "Epoch 28 | Loss: 0.152689, Train_acc: 0.946429, Val_acc: 0.281553, in 2.43s \n",
      "Epoch 29 | Loss: 0.150531, Train_acc: 0.945312, Val_acc: 0.262136, in 2.49s \n",
      "Epoch 30 | Loss: 0.150802, Train_acc: 0.950893, Val_acc: 0.378641, in 2.45s \n",
      "Epoch 31 | Loss: 0.159379, Train_acc: 0.949777, Val_acc: 0.407767, in 2.44s \n",
      "Epoch 32 | Loss: 0.146766, Train_acc: 0.947545, Val_acc: 0.427184, in 2.59s \n",
      "Epoch 33 | Loss: 0.182047, Train_acc: 0.933036, Val_acc: 0.281553, in 2.51s \n",
      "Epoch 34 | Loss: 0.143345, Train_acc: 0.946429, Val_acc: 0.281553, in 2.52s \n",
      "Epoch 35 | Loss: 0.167209, Train_acc: 0.944196, Val_acc: 0.310680, in 2.42s \n",
      "Epoch 36 | Loss: 0.132240, Train_acc: 0.953125, Val_acc: 0.378641, in 2.49s \n",
      "Epoch 37 | Loss: 0.126958, Train_acc: 0.962054, Val_acc: 0.310680, in 2.41s \n",
      "Epoch 38 | Loss: 0.122406, Train_acc: 0.954241, Val_acc: 0.407767, in 2.50s \n",
      "Epoch 39 | Loss: 0.067982, Train_acc: 0.975446, Val_acc: 0.417476, in 2.48s \n",
      "Epoch 40 | Loss: 0.050661, Train_acc: 0.986607, Val_acc: 0.359223, in 2.46s \n",
      "Epoch 41 | Loss: 0.053903, Train_acc: 0.981027, Val_acc: 0.320388, in 2.77s \n",
      "Epoch 42 | Loss: 0.029514, Train_acc: 0.992188, Val_acc: 0.388350, in 2.44s \n",
      "Epoch 43 | Loss: 0.018674, Train_acc: 0.995536, Val_acc: 0.427184, in 2.42s \n",
      "Epoch 44 | Loss: 0.026161, Train_acc: 0.991071, Val_acc: 0.475728, in 2.45s \n",
      "Epoch 45 | Loss: 0.018166, Train_acc: 0.995536, Val_acc: 0.407767, in 2.46s \n",
      "Epoch 46 | Loss: 0.020049, Train_acc: 0.993304, Val_acc: 0.398058, in 2.46s \n",
      "Epoch 47 | Loss: 0.023533, Train_acc: 0.993304, Val_acc: 0.475728, in 2.44s \n",
      "Epoch 48 | Loss: 0.016141, Train_acc: 0.992188, Val_acc: 0.417476, in 2.43s \n",
      "Epoch 49 | Loss: 0.015836, Train_acc: 0.995536, Val_acc: 0.378641, in 2.40s \n",
      "Epoch 50 | Loss: 0.006542, Train_acc: 1.000000, Val_acc: 0.407767, in 2.48s \n",
      "----------------------------------------------------------------------\n",
      "CPU times: user 8min 15s, sys: 10.3 s, total: 8min 25s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Initializing time related variables and lists (to make it easier for metric outputs)\n",
    "\n",
    "# initializing the training times\n",
    "tic_total_train = time.time()\n",
    "epoch_times = []\n",
    "\n",
    "epochs = 50 # for full dataset\n",
    "\n",
    "num_examples = len(train_ds)  #should return 45000 for full ds and 921 for proof-of-concept ds\n",
    "\n",
    "# defining the accuracy evaluation metric\n",
    "metric = mx.metric.Accuracy()\n",
    "\n",
    "# Loss function\n",
    "softmax_ce = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tic_train_epoch = time.time()\n",
    "    # creating cumulative loss variable\n",
    "    cum_loss = 0\n",
    "    # Resetting train_data iterator\n",
    "\n",
    "    \n",
    "    # Looping over train_data iterator\n",
    "    for data, label in train_data:\n",
    "        \n",
    "        # Inside training scope\n",
    "        with ag.record():\n",
    "            # Inputting the data into the nn\n",
    "            outputs = net(data)\n",
    "            \n",
    "            # outputs =outputs.argmax(axis=1)\n",
    "            # label = label.astype('float32').mean().asscalar()\n",
    "            # # Computing the loss\n",
    "            loss = softmax_ce(outputs,label)\n",
    "\n",
    "        # Backpropogating the error\n",
    "        loss.backward()\n",
    "    \n",
    "        # Summation of loss (divided by sample_size in the end)\n",
    "        cum_loss += nd.sum(loss).asscalar()\n",
    "        metric.update(label,outputs)\n",
    "\n",
    "        trainer.step(batch_size)\n",
    "    \n",
    "    # Get evaluation results    \n",
    "    name, acc = metric.get()  \n",
    "    metric.reset()\n",
    "    toc_train_epoch = time.time()\n",
    "    epoch_times.append(toc_train_epoch - tic_train_epoch)\n",
    "    \n",
    "    ## CROSS VALIDATION DATASET\n",
    "    # Looping over cv_data iterator\n",
    "    tic_val = time.time() # initializing cv timer\n",
    "    for data, label in cv_data:\n",
    "        val_outputs = net(data)\n",
    "        \n",
    "        metric.update(label, val_outputs)\n",
    "    \n",
    "    # Getting evaluation results for cv dataset\n",
    "    name, val_acc = metric.get()\n",
    "    metric.reset()\n",
    "    \n",
    "    # Evaluating time elapse between the cv dataset\n",
    "    toc_val = time.time() \n",
    "    \n",
    "    print(\"Epoch %s | Loss: %.6f, Train_acc: %.6f, Val_acc: %.6f, in %.2fs \" %\n",
    "    (epoch+1, cum_loss/num_examples, acc, val_acc, epoch_times[epoch]))\n",
    "print(\"-\"*70)\n",
    "toc_total_train = time.time() # total training time\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Running CNN model on Hold-Out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_acc:  0.3729\n"
     ]
    }
   ],
   "source": [
    "metric = mx.metric.Accuracy()\n",
    "\n",
    "# Looping over cv_data iterator\n",
    "for data, label in test_data:\n",
    "    test_outputs = net(data)\n",
    "    \n",
    "    metric.update(label, test_outputs)\n",
    "\n",
    "# Getting evaluation results for cv dataset\n",
    "name, test_acc = metric.get()\n",
    "print('Test_acc: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export JSON file \n",
    "\n",
    "metrics = {\n",
    "\n",
    "    'model_name': 'ResNetV2-20',\n",
    "    'framework_name': 'MxNet',\n",
    "    'dataset': 'CIFAR-10',\n",
    "    'task': 'classification',\n",
    "    'total_training_time': toc_total_train - tic_total_train, #s\n",
    "    'average_epoch_training_time': np.average(epoch_times), #s\n",
    "    'average_batch_inference_time': 1000*np.average(toc_val - tic_val)/math.ceil(len(cv_ds)/batch_size), #ms\n",
    "    'final_training_loss': cum_loss/num_examples, \n",
    "    'final_evaluation_accuracy': val_acc, \n",
    "    'final_test_accuracy': test_acc \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the metrics file\n",
    "with open('m2-mxnet-cnn.json', 'w') as outfile:\n",
    "    json.dump(metrics, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'ResNetV2-20',\n",
       " 'framework_name': 'MxNet',\n",
       " 'dataset': 'CIFAR-10',\n",
       " 'task': 'classification',\n",
       " 'total_training_time': 132.5778625011444,\n",
       " 'average_epoch_training_time': 2.5639035892486572,\n",
       " 'average_batch_inference_time': 37.78648376464844,\n",
       " 'final_training_loss': 0.006541617683880233,\n",
       " 'final_evaluation_accuracy': 0.4077669902912621,\n",
       " 'final_test_accuracy': 0.3729}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
