{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "717962b8",
   "metadata": {},
   "source": [
    "# Milestone 2: Covolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f8b80",
   "metadata": {},
   "source": [
    "Making a ResNetV2-20 model to perform the CIFAR-10 image classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc567535",
   "metadata": {},
   "source": [
    "## Model Specfications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce7939",
   "metadata": {},
   "source": [
    "Model: ResNetV2-20\n",
    "- Input layer: Input size: (32 x 32) x 3\n",
    "    - conv2d (3 x 3) x 64\n",
    "- ResBlock 1: Input size (32 x 32) x 64\n",
    "     - conv2d (3 x 3) x 16\n",
    "     - conv2d (3 x 3) x 16\n",
    "- ResBlock 2: Input size (32 x 32) x 16\n",
    "     - conv2d (3 x 3) x 16\n",
    "     - conv2d (3 x 3) x 16\n",
    "- ResBlock 3: Input size (32 x 32) x 16\n",
    "     - conv2d (3 x 3) x 16\n",
    "     - conv2d (3 x 3) x 16  \n",
    "- ResBlock 4: Input size (32 x 32) x 16\n",
    "     - conv2d (3 x 3) x 32, stride 2\n",
    "     - conv2d (3 x 3) x 32\n",
    "- ResBlock 5: Input size (16 x 16) x 32\n",
    "     - conv2d (3 x 3) x 32\n",
    "     - conv2d (3 x 3) x 32\n",
    "- ResBlock 6: Input size (16 x 16) x 32\n",
    "     - conv2d (3 x 3) x 32\n",
    "     - conv2d (3 x 3) x 32\n",
    "- ResBlock 7: Input size (16 x 16) x 32\n",
    "     - conv2d (3 x 3) x 64, stride 2\n",
    "     - conv2d (3 x 3) x 64\n",
    "- ResBlock 8: Input size (8 x 8) x 64\n",
    "     - conv2d (3 x 3) x 64\n",
    "     - conv2d (3 x 3) x 64\n",
    "- ResBlock 9: Input size (8 x 8) x 64\n",
    "     - conv2d (3 x 3) x 64\n",
    "     - conv2d (3 x 3) x 64\n",
    "- Pooling: input size (8 x 8) x 64\n",
    "     - GlobalAveragePooling/AdaptiveAveragePooling((1,1))\n",
    "- Output layer: Input size (64,)\n",
    "     - Dense/Linear (64,10)\n",
    "     - Activation: Softmax\n",
    "\n",
    "\n",
    "\n",
    "Data: CIFAR-10 tiny images\n",
    "- 32 x 32 x 3 RGB colour images\n",
    "- Train/Test split: Use data splits already given (50,000 train, 10,000 test). From the 50,000 train images, use 45,000 for training and 5,000 for validation every epoch inside the training loop. Reserve the 10,000 test set images for final evaluation.\n",
    "- Pre-processing inputs: \n",
    "     - Depending on data source, scale int8 inputs to [0, 1] by dividing by 255\n",
    "     - ImageNet normalization \n",
    "          - From the RGB channels, subtract means [0.485, 0.456, 0.406] and divide by standard deviations [0.229, 0.224, 0.225]\n",
    "     - 4 pixel padding on the side, then apply 32x32 crop randomly sampled from the padded image or its horizontal flip as in Section 3.2 of [3]\n",
    "- Preprocessing labels: Use integer indices\n",
    "\n",
    "\n",
    "Hyperparameters:\n",
    "- Optimizer: AdamW\n",
    "- learning rate: 1e-3 \n",
    "- beta_1: 0.9\n",
    "- beta_2: 0.999\n",
    "- weight decay: 0.0001\n",
    "- Number of epochs for training: 50 (TBD)\n",
    "- Batch size: 256 (TBD)\n",
    "\n",
    "\n",
    "Metrics to record:\n",
    "- Total training time (from start of training script to end of training run)\n",
    "- Training time per 1 epoch (measure from start to end of each epoch and average over all epochs)\n",
    "- Inference time per batch (measure per batch and average over all batches)\n",
    "- Last epoch training loss\n",
    "- Last epoch eval accuracy (from the 5,000 evaluation dataset)\n",
    "- Held-out test set accuracy (from the 10,000 test dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b51afea",
   "metadata": {},
   "source": [
    "#### Importing different libraries needed for model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 15:26:45.826626: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-21 15:26:46.551434: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-21 15:26:46.651287: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mxnet/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-12-21 15:26:46.651365: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-21 15:26:46.919067: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-21 15:26:48.659312: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mxnet/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-12-21 15:26:48.659926: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mxnet/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-12-21 15:26:48.659941: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Necessary Libraries\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, autograd as ag, npx\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms, CIFAR10\n",
    "import gluoncv\n",
    "from gluoncv.data import transforms as gcv_transforms\n",
    "\n",
    "# Dataset libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# json library neded to export metrics \n",
    "import json\n",
    "import time\n",
    "\n",
    "import matplotlib as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT if multiple gpus\n",
    "# # number of GPUs to use\n",
    "# num_gpus = 1\n",
    "# ctx = [mx.gpu(i) for i in range(num_gpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels just for reference\n",
    "labels = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\"\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Importing the CIFAR-10 dataset + pre-processing </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([ gcv_transforms.RandomCrop(32, pad=2), # Randomly crop an area and resize it to be 32x32, then pad it to be 40x40 \n",
    "                                    transforms.RandomFlipLeftRight(), # Applying a random horizontal flip\n",
    "                                    transforms.ToTensor(), # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "                                                           # and map values from [0, 255] to [0,1]\n",
    "                                    # Normalize the image with mean and standard deviation calculated across all images\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "                                ])\n",
    "\n",
    "# Since training dataset provides more randomized data (and should be more generalizable), i will not be performing the random operations on the testing dataset.\n",
    "transform_test = transforms.Compose([transforms.ToTensor(),# Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "                                                           # and map values from [0, 255] to [0,1]\n",
    "                                    # Normalize the image with mean and standard deviation calculated across all images\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# USE THIS BATCH SIZE IF MULTIPLE GPUS\n",
    "# # Batch Size for Each GPU\n",
    "# per_device_batch_size = 128\n",
    "# # Number of data loader workers\n",
    "# num_workers = 8\n",
    "# # Calculate effective total batch size\n",
    "# batch_size = per_device_batch_size * num_gpus\n",
    "\n",
    "train_data = gluon.data.DataLoader(\n",
    "    CIFAR10(train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard') # add 'num_workers = num_workers'\n",
    "test_data = gluon.data.DataLoader(\n",
    "    CIFAR10(train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard') # add 'num_workers = num_workers'\n",
    "\n",
    "# will be using- gluon.utils for the rest of the stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.HybridBlock):\n",
    "    def __init__ (self, in_channels, channels, strides = 1 , downsample = False, **kwargs):\n",
    "        super(BasicBlock, self).__init__(**kwargs)\n",
    "        conv_kwargs = {\n",
    "            \"kernel_size\": (3,3),\n",
    "            \"padding\": 1,\n",
    "            \"bias\": False\n",
    "        }\n",
    "        self.strides = strides\n",
    "        self.in_channels = in_channels\n",
    "        self.channels = channels\n",
    "\n",
    "        self.bn1 = nn.BatchNorm()\n",
    "        self.conv1 = nn.Conv2D(channels, kernl_size = 3, strides = strides, padding = 1, use_bias = False, in_channels= in_channels, **conv_kwargs) #verify whether the padding is correct or not\n",
    "        self.bn2 = nn.BatchNorm()\n",
    "        self.conv2 = nn.Conv2D(channels, kernel_size=3, strides = 1, padding = 1, use_bias = False, in_channels=in_channels, **conv_kwargs)\n",
    "        self.relu = nn.Activation('relu')\n",
    "        \n",
    "    def downsample(self,x):\n",
    "    # Downsample with 'nearest' method (this is striding if dims are divisible by stride)\n",
    "    # Equivalently x = x[:, :, ::stride, ::stride].contiguous()   \n",
    "        x = nd.UpSampling(x, type = 'nearest',scale = (1/self.stride) )\n",
    "        #creating padding tenspr for extra channels\n",
    "        (b, c, h, w) = x.shape\n",
    "        num_pad_channels = self.channels - self.in_channels\n",
    "        pad = mx.nd.zeros((b, num_pad_channels, h,w))\n",
    "        # append this padding to the downsampled identity\n",
    "        x = mx.nd.concat((x,pad), dim = 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.strides > 1:\n",
    "            residual = self.downsample(x)\n",
    "        else:\n",
    "            residual = x\n",
    "        x = self.bn1(x)\n",
    "        x = npx.activation(x, act_type='relu')\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.bn2(x)\n",
    "        x = npx.activation(x, act_type='relu')\n",
    "        x = self.conv2(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetV2(nn.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ResNetV2m, self).__init__(**kwargs)\n",
    "\n",
    "        self.input_layer = nn.Conv2D(3,16, (3,3), padding=1)\n",
    "\n",
    "        self.layer_1 = BasicBlock(16,16)\n",
    "        self.layer_2 = BasicBlock(16,16)\n",
    "        self.layer_3 = BasicBlock(16,16)\n",
    "\n",
    "        self.layer_4 = BasicBlock(16,32, strides = 2)\n",
    "        self.layer_5 = BasicBlock(32,32)\n",
    "        self.layer_6 = BasicBlock(32,32)\n",
    "\n",
    "        self.layer_7 = BasicBlock(32,64, strides = 2)\n",
    "        self.layer_8 = BasicBlock(64,64)\n",
    "        self.layer_9 = BasicBlock(64,64)\n",
    "\n",
    "        self.pool = nn.AvgPool2D(pool_size=(1,1), layout = 'NCHW')\n",
    "        self.output_layer = nn.Dense(64,10, activation= 'Softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('mxnet': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3badf9eae0ef155fd65c68fd7214baea2e1574213b204976fda115899cdef650"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
