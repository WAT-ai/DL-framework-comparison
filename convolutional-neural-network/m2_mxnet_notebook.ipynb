{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "717962b8",
   "metadata": {},
   "source": [
    "# Milestone 2: Covolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f8b80",
   "metadata": {},
   "source": [
    "Making a ResNetV2-20 model to perform the CIFAR-10 image classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc567535",
   "metadata": {},
   "source": [
    "## Model Specfications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce7939",
   "metadata": {},
   "source": [
    "Model: ResNetV2-20\n",
    "- Input layer: Input size: (32 x 32) x 3\n",
    "    - conv2d (3 x 3) x 64\n",
    "- ResBlock 1: Input size (32 x 32) x 64\n",
    "     - conv2d (3 x 3) x 16\n",
    "     - conv2d (3 x 3) x 16\n",
    "- ResBlock 2: Input size (32 x 32) x 16\n",
    "     - conv2d (3 x 3) x 16\n",
    "     - conv2d (3 x 3) x 16\n",
    "- ResBlock 3: Input size (32 x 32) x 16\n",
    "     - conv2d (3 x 3) x 16\n",
    "     - conv2d (3 x 3) x 16  \n",
    "- ResBlock 4: Input size (32 x 32) x 16\n",
    "     - conv2d (3 x 3) x 32, stride 2\n",
    "     - conv2d (3 x 3) x 32\n",
    "- ResBlock 5: Input size (16 x 16) x 32\n",
    "     - conv2d (3 x 3) x 32\n",
    "     - conv2d (3 x 3) x 32\n",
    "- ResBlock 6: Input size (16 x 16) x 32\n",
    "     - conv2d (3 x 3) x 32\n",
    "     - conv2d (3 x 3) x 32\n",
    "- ResBlock 7: Input size (16 x 16) x 32\n",
    "     - conv2d (3 x 3) x 64, stride 2\n",
    "     - conv2d (3 x 3) x 64\n",
    "- ResBlock 8: Input size (8 x 8) x 64\n",
    "     - conv2d (3 x 3) x 64\n",
    "     - conv2d (3 x 3) x 64\n",
    "- ResBlock 9: Input size (8 x 8) x 64\n",
    "     - conv2d (3 x 3) x 64\n",
    "     - conv2d (3 x 3) x 64\n",
    "- Pooling: input size (8 x 8) x 64\n",
    "     - GlobalAveragePooling/AdaptiveAveragePooling((1,1))\n",
    "- Output layer: Input size (64,)\n",
    "     - Dense/Linear (64,10)\n",
    "     - Activation: Softmax\n",
    "\n",
    "\n",
    "\n",
    "Data: CIFAR-10 tiny images\n",
    "- 32 x 32 x 3 RGB colour images\n",
    "- Train/Test split: Use data splits already given (50,000 train, 10,000 test). From the 50,000 train images, use 45,000 for training and 5,000 for validation every epoch inside the training loop. Reserve the 10,000 test set images for final evaluation.\n",
    "- Pre-processing inputs: \n",
    "     - Depending on data source, scale int8 inputs to [0, 1] by dividing by 255\n",
    "     - ImageNet normalization \n",
    "          - From the RGB channels, subtract means [0.485, 0.456, 0.406] and divide by standard deviations [0.229, 0.224, 0.225]\n",
    "     - 4 pixel padding on the side, then apply 32x32 crop randomly sampled from the padded image or its horizontal flip as in Section 3.2 of [3]\n",
    "- Preprocessing labels: Use integer indices\n",
    "\n",
    "\n",
    "Hyperparameters:\n",
    "- Optimizer: AdamW\n",
    "- learning rate: 1e-3 \n",
    "- beta_1: 0.9\n",
    "- beta_2: 0.999\n",
    "- weight decay: 0.0001\n",
    "- Number of epochs for training: 50 (TBD)\n",
    "- Batch size: 256 (TBD)\n",
    "\n",
    "\n",
    "Metrics to record:\n",
    "- Total training time (from start of training script to end of training run)\n",
    "- Training time per 1 epoch (measure from start to end of each epoch and average over all epochs)\n",
    "- Inference time per batch (measure per batch and average over all batches)\n",
    "- Last epoch training loss\n",
    "- Last epoch eval accuracy (from the 5,000 evaluation dataset)\n",
    "- Held-out test set accuracy (from the 10,000 test dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b51afea",
   "metadata": {},
   "source": [
    "#### Importing different libraries needed for model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-30 00:07:40.728015: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-30 00:07:41.238885: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-30 00:07:41.315507: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mushi251/environments/.venv/mxnet/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-12-30 00:07:41.315533: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-30 00:07:42.838540: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mushi251/environments/.venv/mxnet/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-12-30 00:07:42.838744: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mushi251/environments/.venv/mxnet/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-12-30 00:07:42.838754: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Necessary Libraries\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, autograd as ag, npx\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms, CIFAR10\n",
    "import gluoncv\n",
    "from gluoncv.data import transforms as gcv_transforms\n",
    "\n",
    "# Dataset libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# json library neded to export metrics \n",
    "import json\n",
    "import time\n",
    "\n",
    "import matplotlib as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT if multiple gpus\n",
    "# # number of GPUs to use\n",
    "# num_gpus = 1\n",
    "# ctx = [mx.gpu(i) for i in range(num_gpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels just for reference\n",
    "labels = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\"\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Importing the CIFAR-10 dataset + pre-processing </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([ gcv_transforms.RandomCrop(32, pad=2), # Randomly crop an area and resize it to be 32x32, then pad it to be 36x36 \n",
    "                                    transforms.RandomFlipLeftRight(), # Applying a random horizontal flip\n",
    "                                    transforms.ToTensor(), # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "                                                           # and map values from [0, 255] to [0,1]\n",
    "                                    # Normalize the image with mean and standard deviation calculated across all images\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "                                ])\n",
    "\n",
    "# Since training dataset provides more randomized data (and should be more generalizable), i will not be performing the random operations on the testing dataset.\n",
    "transform_test = transforms.Compose([transforms.ToTensor(),# Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "                                                           # and map values from [0, 255] to [0,1]\n",
    "                                    # Normalize the image with mean and standard deviation calculated across all images\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# USE THIS BATCH SIZE IF MULTIPLE GPUS\n",
    "# # Batch Size for Each GPU\n",
    "# per_device_batch_size = 128\n",
    "# # Number of data loader workers\n",
    "# num_workers = 8\n",
    "# # Calculate effective total batch size\n",
    "# batch_size = per_device_batch_size * num_gpus\n",
    "\n",
    "train_data = gluon.data.DataLoader(\n",
    "    CIFAR10(train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard') # add 'num_workers = num_workers'\n",
    "test_data = gluon.data.DataLoader(\n",
    "    CIFAR10(train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard') # add 'num_workers = num_workers'\n",
    "\n",
    "# will be using- gluon.utils for the rest of the stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 3, 32, 32) (256,)\n"
     ]
    }
   ],
   "source": [
    "# see data\n",
    "for data, label in train_data:\n",
    "    print(data.shape, label.shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Defining ResNetV2 class structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Defining the Basic Block structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Block):\n",
    "    def __init__ (self, in_channels, channels, strides = 1 , **kwargs):\n",
    "        super(BasicBlock, self).__init__(**kwargs)\n",
    "        conv_kwargs = {\n",
    "            \"kernel_size\": (3,3),\n",
    "            \"padding\": 1,\n",
    "            \"use_bias\": False\n",
    "        }\n",
    "        self.strides = strides\n",
    "        self.in_channels = in_channels\n",
    "        self.channels = channels\n",
    "\n",
    "        self.bn1 = nn.BatchNorm(in_channels= in_channels)        \n",
    "        self.conv1 = nn.Conv2D(channels, strides= strides,  in_channels= in_channels, **conv_kwargs) \n",
    "        \n",
    "        self.bn2 = nn.BatchNorm(in_channels= channels)\n",
    "        self.conv2 = nn.Conv2D(channels, in_channels= channels, **conv_kwargs)\n",
    "        self.relu = nn.Activation('relu')\n",
    "        \n",
    "    def downsample(self,x):\n",
    "    # Downsample with 'nearest' method (this is striding if dims are divisible by stride)\n",
    "    # Equivalently x = x[:, :, ::stride, ::stride].contiguous()   \n",
    "        x = x[:,:, ::self.strides, ::self.strides]\n",
    "        #creating padding tenspr for extra channels\n",
    "        (b, c, h, w) = x.shape\n",
    "        num_pad_channels = self.channels - self.in_channels\n",
    "        pad = mx.nd.zeros((b, num_pad_channels, h,w))\n",
    "        # append this padding to the downsampled identity\n",
    "        x = mx.nd.concat(x , pad, dim = 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.strides > 1:\n",
    "            residual = self.downsample(x)\n",
    "        else:\n",
    "            residual = x\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        return x + residual"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Defining the ResNetV2 CNN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetV2(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ResNetV2, self).__init__(**kwargs)\n",
    "\n",
    "        self.input_layer = nn.Conv2D(in_channels = 3, channels= 16, kernel_size=(3,3), padding=1)\n",
    "\n",
    "        self.layer_1 = BasicBlock(16,16)\n",
    "        self.layer_2 = BasicBlock(16,16)\n",
    "        self.layer_3 = BasicBlock(16,16)\n",
    "\n",
    "        self.layer_4 = BasicBlock(16,32, strides = 2)\n",
    "        self.layer_5 = BasicBlock(32,32)\n",
    "        self.layer_6 = BasicBlock(32,32)\n",
    "\n",
    "        self.layer_7 = BasicBlock(32,64, strides = 2)\n",
    "        self.layer_8 = BasicBlock(64,64)\n",
    "        self.layer_9 = BasicBlock(64,64)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.pool = nn.GlobalAvgPool2D(layout = 'NCHW')\n",
    "        self.output_layer = nn.Dense(units=10, in_units=64)\n",
    "\n",
    "    \n",
    "    def forward (self, x):\n",
    "        out = self.input_layer(x)\n",
    "        out = self.layer_1(out)\n",
    "        out = self.layer_2(out)\n",
    "        out = self.layer_3(out)\n",
    "        out = self.layer_4(out)\n",
    "        out = self.layer_5(out)\n",
    "        out = self.layer_6(out)\n",
    "        out = self.layer_7(out)\n",
    "        out = self.layer_8(out)\n",
    "        out = self.layer_9(out)\n",
    "        # print(\"Before Pool: \", out.shape)\n",
    "        out = self.pool(out)\n",
    "        # print(\"After Pool: \", out.shape)\n",
    "        out = self.flatten(out)\n",
    "        # print(\"After Flattening: \", out.shape)\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNetV2()\n",
    "net.initialize()\n",
    "# net.collect_params\n",
    "# print(net.collect_params)\n",
    "# net.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sanity check to see all the layers\n",
    "# params = net.collect_params()\n",
    "\n",
    "# for key, value in params.items():\n",
    "#     print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(params = net.collect_params(),\n",
    "                    optimizer='adam',\n",
    "                    optimizer_params = {'learning_rate': 0.001, 'beta1': 0.9, 'beta2': 0.999, 'wd':0.0001}\n",
    "                    ) # The guidelines state using AdamW optimizer, unsure whether 'adam' is sufficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = mx.np.random.normal(size=(4, 3, 32, 32)).as_nd_ndarray()\n",
    "outputs = net(inputs) # appears that something is happening and I am getting a NoneType back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m     trainer\u001b[39m.\u001b[39mstep(batch_size)\n\u001b[1;32m     20\u001b[0m     \u001b[39m# calculate training metrics\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mmean()\u001b[39m.\u001b[39;49masscalar()\n\u001b[1;32m     22\u001b[0m     train_acc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m acc(output, label)\n\u001b[1;32m     23\u001b[0m \u001b[39m# calculate validation accuracy\u001b[39;00m\n",
      "File \u001b[0;32m~/environments/.venv/mxnet/lib/python3.8/site-packages/mxnet/ndarray/ndarray.py:2590\u001b[0m, in \u001b[0;36mNDArray.asscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2588\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe current array is not a scalar\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2589\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 2590\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49masnumpy()[\u001b[39m0\u001b[39m]\n\u001b[1;32m   2591\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2592\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39masnumpy()[()]\n",
      "File \u001b[0;32m~/environments/.venv/mxnet/lib/python3.8/site-packages/mxnet/ndarray/ndarray.py:2568\u001b[0m, in \u001b[0;36mNDArray.asnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2551\u001b[0m \u001b[39m\"\"\"Returns a ``numpy.ndarray`` object with value copied from this array.\u001b[39;00m\n\u001b[1;32m   2552\u001b[0m \n\u001b[1;32m   2553\u001b[0m \u001b[39mExamples\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2565\u001b[0m \u001b[39m       [1, 1, 1]], dtype=int32)\u001b[39;00m\n\u001b[1;32m   2566\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2567\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m-> 2568\u001b[0m check_call(_LIB\u001b[39m.\u001b[39;49mMXNDArraySyncCopyToCPU(\n\u001b[1;32m   2569\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   2570\u001b[0m     data\u001b[39m.\u001b[39;49mctypes\u001b[39m.\u001b[39;49mdata_as(ctypes\u001b[39m.\u001b[39;49mc_void_p),\n\u001b[1;32m   2571\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_size_t(data\u001b[39m.\u001b[39;49msize)))\n\u001b[1;32m   2572\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def acc(output, label):\n",
    "    # output: (batch, num_output) float32 ndarray\n",
    "    # label: (batch, ) int32 ndarray\n",
    "    return (output.argmax(axis=1) ==\n",
    "            label.astype('float32')).mean().asscalar()\n",
    "    \n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss, train_acc, valid_acc = 0., 0., 0.\n",
    "    tic = time.time()\n",
    "    for data, label in train_data:\n",
    "        # forward + backward\n",
    "        with ag.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        trainer.step(batch_size)\n",
    "        # calculate training metrics\n",
    "        train_loss += loss.mean().asscalar()\n",
    "        train_acc += acc(output, label)\n",
    "    # calculate validation accuracy\n",
    "    for data, label in test_data:\n",
    "        test_acc += acc(net(data), label)\n",
    "    print(\"Epoch %d: loss %.3f, train acc %.3f, test acc %.3f, in %.1f sec\" % (\n",
    "            epoch, train_loss/len(train_data), train_acc/len(train_data),\n",
    "            test_acc/len(test_data), time.time()-tic))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mxnet",
   "language": "python",
   "name": "mxnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
