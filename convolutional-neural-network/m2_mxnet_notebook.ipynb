{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "717962b8",
   "metadata": {},
   "source": [
    "# Milestone 2: Covolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f8b80",
   "metadata": {},
   "source": [
    "Making a ResNetV2-20 model to perform the CIFAR-10 image classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc567535",
   "metadata": {},
   "source": [
    "## Model Specfications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce7939",
   "metadata": {},
   "source": [
    "Model: ResNetV2-20\n",
    "- Input layer: Input size: (32 x 32) x 3\n",
    "    - conv2d (3 x 3) x 64\n",
    "- ResBlock 1: Input size (32 x 32) x 64\n",
    "     - conv2d (3 x 3) x 16\n",
    "     - conv2d (3 x 3) x 16\n",
    "- ResBlock 2: Input size (32 x 32) x 16\n",
    "     - conv2d (3 x 3) x 16\n",
    "     - conv2d (3 x 3) x 16\n",
    "- ResBlock 3: Input size (32 x 32) x 16\n",
    "     - conv2d (3 x 3) x 16\n",
    "     - conv2d (3 x 3) x 16  \n",
    "- ResBlock 4: Input size (32 x 32) x 16\n",
    "     - conv2d (3 x 3) x 32, stride 2\n",
    "     - conv2d (3 x 3) x 32\n",
    "- ResBlock 5: Input size (16 x 16) x 32\n",
    "     - conv2d (3 x 3) x 32\n",
    "     - conv2d (3 x 3) x 32\n",
    "- ResBlock 6: Input size (16 x 16) x 32\n",
    "     - conv2d (3 x 3) x 32\n",
    "     - conv2d (3 x 3) x 32\n",
    "- ResBlock 7: Input size (16 x 16) x 32\n",
    "     - conv2d (3 x 3) x 64, stride 2\n",
    "     - conv2d (3 x 3) x 64\n",
    "- ResBlock 8: Input size (8 x 8) x 64\n",
    "     - conv2d (3 x 3) x 64\n",
    "     - conv2d (3 x 3) x 64\n",
    "- ResBlock 9: Input size (8 x 8) x 64\n",
    "     - conv2d (3 x 3) x 64\n",
    "     - conv2d (3 x 3) x 64\n",
    "- Pooling: input size (8 x 8) x 64\n",
    "     - GlobalAveragePooling/AdaptiveAveragePooling((1,1))\n",
    "- Output layer: Input size (64,)\n",
    "     - Dense/Linear (64,10)\n",
    "     - Activation: Softmax\n",
    "\n",
    "\n",
    "\n",
    "Data: CIFAR-10 tiny images\n",
    "- 32 x 32 x 3 RGB colour images\n",
    "- Train/Test split: Use data splits already given (50,000 train, 10,000 test). From the 50,000 train images, use 45,000 for training and 5,000 for validation every epoch inside the training loop. Reserve the 10,000 test set images for final evaluation.\n",
    "- Pre-processing inputs: \n",
    "     - Depending on data source, scale int8 inputs to [0, 1] by dividing by 255\n",
    "     - ImageNet normalization \n",
    "          - From the RGB channels, subtract means [0.485, 0.456, 0.406] and divide by standard deviations [0.229, 0.224, 0.225]\n",
    "     - 4 pixel padding on the side, then apply 32x32 crop randomly sampled from the padded image or its horizontal flip as in Section 3.2 of [3]\n",
    "- Preprocessing labels: Use integer indices\n",
    "\n",
    "\n",
    "Hyperparameters:\n",
    "- Optimizer: AdamW\n",
    "- learning rate: 1e-3 \n",
    "- beta_1: 0.9\n",
    "- beta_2: 0.999\n",
    "- weight decay: 0.0001\n",
    "- Number of epochs for training: 50 (TBD)\n",
    "- Batch size: 256 (TBD)\n",
    "\n",
    "\n",
    "Metrics to record:\n",
    "- Total training time (from start of training script to end of training run)\n",
    "- Training time per 1 epoch (measure from start to end of each epoch and average over all epochs)\n",
    "- Inference time per batch (measure per batch and average over all batches)\n",
    "- Last epoch training loss\n",
    "- Last epoch eval accuracy (from the 5,000 evaluation dataset)\n",
    "- Held-out test set accuracy (from the 10,000 test dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b51afea",
   "metadata": {},
   "source": [
    "#### Importing different libraries needed for model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-29 16:58:35.153921: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-29 16:58:35.411816: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-29 16:58:35.419820: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mushi251/environments/.venv/mxnet/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-12-29 16:58:35.419840: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-29 16:58:36.272963: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mushi251/environments/.venv/mxnet/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-12-29 16:58:36.273078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mushi251/environments/.venv/mxnet/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-12-29 16:58:36.273085: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Necessary Libraries\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, autograd as ag, npx\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms, CIFAR10\n",
    "import gluoncv\n",
    "from gluoncv.data import transforms as gcv_transforms\n",
    "\n",
    "# Dataset libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# json library neded to export metrics \n",
    "import json\n",
    "import time\n",
    "\n",
    "import matplotlib as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT if multiple gpus\n",
    "# # number of GPUs to use\n",
    "# num_gpus = 1\n",
    "# ctx = [mx.gpu(i) for i in range(num_gpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels just for reference\n",
    "labels = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\"\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Importing the CIFAR-10 dataset + pre-processing </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([ gcv_transforms.RandomCrop(32, pad=2), # Randomly crop an area and resize it to be 32x32, then pad it to be 36x36 \n",
    "                                    transforms.RandomFlipLeftRight(), # Applying a random horizontal flip\n",
    "                                    transforms.ToTensor(), # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "                                                           # and map values from [0, 255] to [0,1]\n",
    "                                    # Normalize the image with mean and standard deviation calculated across all images\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "                                ])\n",
    "\n",
    "# Since training dataset provides more randomized data (and should be more generalizable), i will not be performing the random operations on the testing dataset.\n",
    "transform_test = transforms.Compose([transforms.ToTensor(),# Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "                                                           # and map values from [0, 255] to [0,1]\n",
    "                                    # Normalize the image with mean and standard deviation calculated across all images\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# USE THIS BATCH SIZE IF MULTIPLE GPUS\n",
    "# # Batch Size for Each GPU\n",
    "# per_device_batch_size = 128\n",
    "# # Number of data loader workers\n",
    "# num_workers = 8\n",
    "# # Calculate effective total batch size\n",
    "# batch_size = per_device_batch_size * num_gpus\n",
    "\n",
    "train_data = gluon.data.DataLoader(\n",
    "    CIFAR10(train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard') # add 'num_workers = num_workers'\n",
    "test_data = gluon.data.DataLoader(\n",
    "    CIFAR10(train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard') # add 'num_workers = num_workers'\n",
    "\n",
    "# will be using- gluon.utils for the rest of the stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Block):\n",
    "    def __init__ (self, in_channels, channels, strides = 1 , **kwargs):\n",
    "        super(BasicBlock, self).__init__(**kwargs)\n",
    "        conv_kwargs = {\n",
    "            \"kernel_size\": (3,3),\n",
    "            \"padding\": 1,\n",
    "            \"use_bias\": False\n",
    "        }\n",
    "        self.strides = strides\n",
    "        self.in_channels = in_channels\n",
    "        self.channels = channels\n",
    "\n",
    "        self.bn1 = nn.BatchNorm(in_channels= in_channels)        \n",
    "        self.conv1 = nn.Conv2D(channels, strides= strides,  in_channels= in_channels, **conv_kwargs) \n",
    "        \n",
    "        self.bn2 = nn.BatchNorm(in_channels= channels)\n",
    "        self.conv2 = nn.Conv2D(channels, in_channels= channels, **conv_kwargs)\n",
    "        self.relu = nn.Activation('relu')\n",
    "        \n",
    "    def downsample(self,x):\n",
    "    # Downsample with 'nearest' method (this is striding if dims are divisible by stride)\n",
    "    # Equivalently x = x[:, :, ::stride, ::stride].contiguous()   \n",
    "        x = x[:,:, ::self.strides, ::self.strides]\n",
    "        #creating padding tenspr for extra channels\n",
    "        (b, c, h, w) = x.shape\n",
    "        num_pad_channels = self.channels - self.in_channels\n",
    "        pad = mx.nd.zeros((b, num_pad_channels, h,w))\n",
    "        # append this padding to the downsampled identity\n",
    "        x = mx.nd.concat(x , pad, dim = 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.strides > 1:\n",
    "            residual = self.downsample(x)\n",
    "        else:\n",
    "            residual = x\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetV2(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ResNetV2, self).__init__(**kwargs)\n",
    "\n",
    "        self.input_layer = nn.Conv2D(in_channels = 3, channels= 16, kernel_size=(3,3), padding=1)\n",
    "\n",
    "        self.layer_1 = BasicBlock(16,16)\n",
    "        self.layer_2 = BasicBlock(16,16)\n",
    "        self.layer_3 = BasicBlock(16,16)\n",
    "\n",
    "        self.layer_4 = BasicBlock(16,32, strides = 2)\n",
    "        self.layer_5 = BasicBlock(32,32)\n",
    "        self.layer_6 = BasicBlock(32,32)\n",
    "\n",
    "        self.layer_7 = BasicBlock(32,64, strides = 2)\n",
    "        self.layer_8 = BasicBlock(64,64)\n",
    "        self.layer_9 = BasicBlock(64,64)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.pool = nn.AvgPool2D(layout = 'NCHW')\n",
    "        self.output_layer = nn.Dense(in_units = 64, units = 10)\n",
    "\n",
    "    \n",
    "    def forward (self, x):\n",
    "        out = self.input_layer(x)\n",
    "        out = self.layer_1(out)\n",
    "        out = self.layer_2(out)\n",
    "        out = self.layer_3(out)\n",
    "        out = self.layer_4(out)\n",
    "        out = self.layer_5(out)\n",
    "        out = self.layer_6(out)\n",
    "        out = self.layer_7(out)\n",
    "        out = self.layer_8(out)\n",
    "        out = self.layer_9(out)\n",
    "        out = self.pool(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.output_layer(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Block.collect_params of ResNetV2(\n",
      "  (input_layer): Conv2D(3 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (layer_1): BasicBlock(\n",
      "    (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=16)\n",
      "    (conv1): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=16)\n",
      "    (conv2): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (relu): Activation(relu)\n",
      "  )\n",
      "  (layer_2): BasicBlock(\n",
      "    (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=16)\n",
      "    (conv1): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=16)\n",
      "    (conv2): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (relu): Activation(relu)\n",
      "  )\n",
      "  (layer_3): BasicBlock(\n",
      "    (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=16)\n",
      "    (conv1): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=16)\n",
      "    (conv2): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (relu): Activation(relu)\n",
      "  )\n",
      "  (layer_4): BasicBlock(\n",
      "    (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=16)\n",
      "    (conv1): Conv2D(16 -> 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=32)\n",
      "    (conv2): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (relu): Activation(relu)\n",
      "  )\n",
      "  (layer_5): BasicBlock(\n",
      "    (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=32)\n",
      "    (conv1): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=32)\n",
      "    (conv2): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (relu): Activation(relu)\n",
      "  )\n",
      "  (layer_6): BasicBlock(\n",
      "    (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=32)\n",
      "    (conv1): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=32)\n",
      "    (conv2): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (relu): Activation(relu)\n",
      "  )\n",
      "  (layer_7): BasicBlock(\n",
      "    (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=32)\n",
      "    (conv1): Conv2D(32 -> 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "    (conv2): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (relu): Activation(relu)\n",
      "  )\n",
      "  (layer_8): BasicBlock(\n",
      "    (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "    (conv1): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "    (conv2): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (relu): Activation(relu)\n",
      "  )\n",
      "  (layer_9): BasicBlock(\n",
      "    (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "    (conv1): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "    (conv2): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (relu): Activation(relu)\n",
      "  )\n",
      "  (flatten): Flatten\n",
      "  (pool): AvgPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)\n",
      "  (output_layer): Dense(64 -> 10, linear)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "net = ResNetV2()\n",
    "net.initialize()\n",
    "net.collect_params\n",
    "print(net.collect_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv57_weight Parameter conv57_weight (shape=(16, 3, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "conv57_bias Parameter conv57_bias (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm54_gamma Parameter batchnorm54_gamma (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm54_beta Parameter batchnorm54_beta (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm54_running_mean Parameter batchnorm54_running_mean (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm54_running_var Parameter batchnorm54_running_var (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "conv58_weight Parameter conv58_weight (shape=(16, 16, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "batchnorm55_gamma Parameter batchnorm55_gamma (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm55_beta Parameter batchnorm55_beta (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm55_running_mean Parameter batchnorm55_running_mean (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm55_running_var Parameter batchnorm55_running_var (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "conv59_weight Parameter conv59_weight (shape=(16, 16, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "batchnorm56_gamma Parameter batchnorm56_gamma (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm56_beta Parameter batchnorm56_beta (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm56_running_mean Parameter batchnorm56_running_mean (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm56_running_var Parameter batchnorm56_running_var (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "conv60_weight Parameter conv60_weight (shape=(16, 16, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "batchnorm57_gamma Parameter batchnorm57_gamma (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm57_beta Parameter batchnorm57_beta (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm57_running_mean Parameter batchnorm57_running_mean (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm57_running_var Parameter batchnorm57_running_var (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "conv61_weight Parameter conv61_weight (shape=(16, 16, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "batchnorm58_gamma Parameter batchnorm58_gamma (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm58_beta Parameter batchnorm58_beta (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm58_running_mean Parameter batchnorm58_running_mean (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm58_running_var Parameter batchnorm58_running_var (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "conv62_weight Parameter conv62_weight (shape=(16, 16, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "batchnorm59_gamma Parameter batchnorm59_gamma (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm59_beta Parameter batchnorm59_beta (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm59_running_mean Parameter batchnorm59_running_mean (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm59_running_var Parameter batchnorm59_running_var (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "conv63_weight Parameter conv63_weight (shape=(16, 16, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "batchnorm60_gamma Parameter batchnorm60_gamma (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm60_beta Parameter batchnorm60_beta (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm60_running_mean Parameter batchnorm60_running_mean (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm60_running_var Parameter batchnorm60_running_var (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "conv64_weight Parameter conv64_weight (shape=(32, 16, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "batchnorm61_gamma Parameter batchnorm61_gamma (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm61_beta Parameter batchnorm61_beta (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm61_running_mean Parameter batchnorm61_running_mean (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm61_running_var Parameter batchnorm61_running_var (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "conv65_weight Parameter conv65_weight (shape=(32, 32, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "batchnorm62_gamma Parameter batchnorm62_gamma (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm62_beta Parameter batchnorm62_beta (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm62_running_mean Parameter batchnorm62_running_mean (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm62_running_var Parameter batchnorm62_running_var (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "conv66_weight Parameter conv66_weight (shape=(32, 32, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "batchnorm63_gamma Parameter batchnorm63_gamma (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm63_beta Parameter batchnorm63_beta (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm63_running_mean Parameter batchnorm63_running_mean (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm63_running_var Parameter batchnorm63_running_var (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "conv67_weight Parameter conv67_weight (shape=(32, 32, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "batchnorm64_gamma Parameter batchnorm64_gamma (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm64_beta Parameter batchnorm64_beta (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm64_running_mean Parameter batchnorm64_running_mean (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm64_running_var Parameter batchnorm64_running_var (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "conv68_weight Parameter conv68_weight (shape=(32, 32, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "batchnorm65_gamma Parameter batchnorm65_gamma (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm65_beta Parameter batchnorm65_beta (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm65_running_mean Parameter batchnorm65_running_mean (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm65_running_var Parameter batchnorm65_running_var (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "conv69_weight Parameter conv69_weight (shape=(32, 32, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "batchnorm66_gamma Parameter batchnorm66_gamma (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm66_beta Parameter batchnorm66_beta (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm66_running_mean Parameter batchnorm66_running_mean (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm66_running_var Parameter batchnorm66_running_var (shape=(32,), dtype=<class 'numpy.float32'>)\n",
      "conv70_weight Parameter conv70_weight (shape=(64, 32, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "batchnorm67_gamma Parameter batchnorm67_gamma (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm67_beta Parameter batchnorm67_beta (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm67_running_mean Parameter batchnorm67_running_mean (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm67_running_var Parameter batchnorm67_running_var (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "conv71_weight Parameter conv71_weight (shape=(64, 64, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "batchnorm68_gamma Parameter batchnorm68_gamma (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm68_beta Parameter batchnorm68_beta (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm68_running_mean Parameter batchnorm68_running_mean (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm68_running_var Parameter batchnorm68_running_var (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "conv72_weight Parameter conv72_weight (shape=(64, 64, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "batchnorm69_gamma Parameter batchnorm69_gamma (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm69_beta Parameter batchnorm69_beta (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm69_running_mean Parameter batchnorm69_running_mean (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm69_running_var Parameter batchnorm69_running_var (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "conv73_weight Parameter conv73_weight (shape=(64, 64, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "batchnorm70_gamma Parameter batchnorm70_gamma (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm70_beta Parameter batchnorm70_beta (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm70_running_mean Parameter batchnorm70_running_mean (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm70_running_var Parameter batchnorm70_running_var (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "conv74_weight Parameter conv74_weight (shape=(64, 64, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "batchnorm71_gamma Parameter batchnorm71_gamma (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm71_beta Parameter batchnorm71_beta (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm71_running_mean Parameter batchnorm71_running_mean (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "batchnorm71_running_var Parameter batchnorm71_running_var (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "conv75_weight Parameter conv75_weight (shape=(64, 64, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "dense3_weight Parameter dense3_weight (shape=(10, 64), dtype=float32)\n",
      "dense3_bias Parameter dense3_bias (shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "params = net.collect_params()\n",
    "\n",
    "for key, value in params.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(params = net.collect_params(),\n",
    "                    optimizer='adam',\n",
    "                    optimizer_params = {'learning_rate': 0.001, 'beta1': 0.9, 'beta2': 0.999, 'wd':0.0001}\n",
    "                    ) # The guidelines state using AdamW optimizer, unsure whether 'adam' is sufficient\n",
    "\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "MXNetError",
     "evalue": "MXNetError: Shape inconsistent, Provided = [10,64], inferred shape=(10,1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m net\u001b[39m.\u001b[39minitialize()\n\u001b[1;32m      3\u001b[0m inputs \u001b[39m=\u001b[39m mx\u001b[39m.\u001b[39mnp\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(size\u001b[39m=\u001b[39m(\u001b[39m4\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m))\u001b[39m.\u001b[39mas_nd_ndarray()\n\u001b[0;32m----> 4\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs)\n",
      "File \u001b[0;32m~/environments/.venv/mxnet/lib/python3.8/site-packages/mxnet/gluon/block.py:825\u001b[0m, in \u001b[0;36mBlock.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    823\u001b[0m     hook(\u001b[39mself\u001b[39m, args)\n\u001b[0;32m--> 825\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    827\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    828\u001b[0m     hook(\u001b[39mself\u001b[39m, args, out)\n",
      "Cell \u001b[0;32mIn[12], line 39\u001b[0m, in \u001b[0;36mResNetV2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(out)\n\u001b[1;32m     38\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(out)\n\u001b[0;32m---> 39\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_layer(out)\n",
      "File \u001b[0;32m~/environments/.venv/mxnet/lib/python3.8/site-packages/mxnet/gluon/block.py:825\u001b[0m, in \u001b[0;36mBlock.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    823\u001b[0m     hook(\u001b[39mself\u001b[39m, args)\n\u001b[0;32m--> 825\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    827\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    828\u001b[0m     hook(\u001b[39mself\u001b[39m, args, out)\n",
      "File \u001b[0;32m~/environments/.venv/mxnet/lib/python3.8/site-packages/mxnet/gluon/block.py:1502\u001b[0m, in \u001b[0;36mHybridBlock.forward\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 v\u001b[39m.\u001b[39m_finish_deferred_init()\n\u001b[1;32m   1500\u001b[0m             params \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mdata(ctx) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reg_params\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m-> 1502\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhybrid_forward(ndarray, x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m   1503\u001b[0m params \u001b[39m=\u001b[39m {i: j\u001b[39m.\u001b[39mvar() \u001b[39mfor\u001b[39;00m i, j \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reg_params\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m   1504\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname_scope():\n",
      "File \u001b[0;32m~/environments/.venv/mxnet/lib/python3.8/site-packages/mxnet/gluon/nn/basic_layers.py:224\u001b[0m, in \u001b[0;36mDense.hybrid_forward\u001b[0;34m(self, F, x, weight, bias)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhybrid_forward\u001b[39m(\u001b[39mself\u001b[39m, F, x, weight, bias\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    223\u001b[0m     fc \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnpx\u001b[39m.\u001b[39mfully_connected \u001b[39mif\u001b[39;00m is_np_array() \u001b[39melse\u001b[39;00m F\u001b[39m.\u001b[39mFullyConnected\n\u001b[0;32m--> 224\u001b[0m     act \u001b[39m=\u001b[39m fc(x, weight, bias, no_bias\u001b[39m=\u001b[39;49mbias \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, num_hidden\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_units,\n\u001b[1;32m    225\u001b[0m              flatten\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flatten, name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfwd\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    226\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m         act \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(act)\n",
      "File \u001b[0;32m<string>:86\u001b[0m, in \u001b[0;36mFullyConnected\u001b[0;34m(data, weight, bias, num_hidden, no_bias, flatten, out, name, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/environments/.venv/mxnet/lib/python3.8/site-packages/mxnet/_ctypes/ndarray.py:82\u001b[0m, in \u001b[0;36m_imperative_invoke\u001b[0;34m(handle, ndargs, keys, vals, out, is_np_op, output_is_list)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39m# return output stypes to avoid the c_api call for checking\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m# a handle's stype in _ndarray_cls\u001b[39;00m\n\u001b[1;32m     80\u001b[0m out_stypes \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mPOINTER(ctypes\u001b[39m.\u001b[39mc_int)()\n\u001b[0;32m---> 82\u001b[0m check_call(_LIB\u001b[39m.\u001b[39;49mMXImperativeInvokeEx(\n\u001b[1;32m     83\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_void_p(handle),\n\u001b[1;32m     84\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(\u001b[39mlen\u001b[39;49m(ndargs)),\n\u001b[1;32m     85\u001b[0m     c_handle_array(ndargs),\n\u001b[1;32m     86\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(num_output),\n\u001b[1;32m     87\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(output_vars),\n\u001b[1;32m     88\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(\u001b[39mlen\u001b[39;49m(keys)),\n\u001b[1;32m     89\u001b[0m     c_str_array(keys),\n\u001b[1;32m     90\u001b[0m     c_str_array([\u001b[39mstr\u001b[39;49m(s) \u001b[39mfor\u001b[39;49;00m s \u001b[39min\u001b[39;49;00m vals]),\n\u001b[1;32m     91\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(out_stypes)))\n\u001b[1;32m     93\u001b[0m create_ndarray_fn \u001b[39m=\u001b[39m _global_var\u001b[39m.\u001b[39m_np_ndarray_cls \u001b[39mif\u001b[39;00m is_np_op \u001b[39melse\u001b[39;00m _global_var\u001b[39m.\u001b[39m_ndarray_cls\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m original_output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/environments/.venv/mxnet/lib/python3.8/site-packages/mxnet/base.py:246\u001b[0m, in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39m\"\"\"Check the return value of C API call.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \n\u001b[1;32m    237\u001b[0m \u001b[39mThis function will raise an exception when an error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39m    return value from API calls.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 246\u001b[0m     \u001b[39mraise\u001b[39;00m get_last_ffi_error()\n",
      "\u001b[0;31mMXNetError\u001b[0m: MXNetError: Shape inconsistent, Provided = [10,64], inferred shape=(10,1024)"
     ]
    }
   ],
   "source": [
    "net = ResNetV2()\n",
    "net.initialize()\n",
    "inputs = mx.np.random.normal(size=(4, 3, 32, 32)).as_nd_ndarray()\n",
    "outputs = net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mxnet",
   "language": "python",
   "name": "mxnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
