{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, GlobalAveragePooling2D, Dense\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityResidual(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_channels, stride):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        b, h, w, in_channels = input_shape\n",
    "        self.in_channels = in_channels\n",
    "        self.h = h // self.stride\n",
    "        self.w = w // self.stride\n",
    "        self.b = b\n",
    "        self.c = self.out_channels - self.in_channels\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        # Downsample spatially\n",
    "        x = input_tensor[:, ::self.stride, ::self.stride, :]\n",
    "        # Create padding tensor for extra channels \n",
    "        if self.out_channels != self.in_channels:\n",
    "            pad = tf.zeros((self.b, self.h, self.w, self.c))\n",
    "            # Append padding to the downsampled identity\n",
    "            x = tf.concat((x, pad), axis=-1)\n",
    "        return x\n",
    "\n",
    "class ResNetV2Layer(tf.keras.Model):\n",
    "    def __init__(self, channels, stride=1):\n",
    "        super().__init__()\n",
    "        conv_kwargs = {\n",
    "            \"padding\": \"same\",\n",
    "            \"use_bias\": False\n",
    "        }\n",
    "        self.stride = stride\n",
    "        self.channels = channels\n",
    "        self.relu = tf.nn.relu\n",
    "        self.residual = IdentityResidual(channels, stride)\n",
    "        self.conv1 = Conv2D(filters=channels, kernel_size=3, strides=self.stride, **conv_kwargs)\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.conv2 = Conv2D(filters=channels, kernel_size=3, **conv_kwargs)\n",
    "        self.bn2 = BatchNormalization()\n",
    "    \n",
    "    def call(self, input_tensor, training=False):\n",
    "        residual = self.residual(input_tensor)\n",
    "        x = self.bn1(input_tensor, training=training)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 16, 16, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = ResNetV2Layer(16)\n",
    "layer_2 = ResNetV2Layer(32, stride=2)\n",
    "inputs = tf.random.normal((4, 32, 32, 3))\n",
    "z = layer(inputs)\n",
    "z = layer_2(z)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 16, 16, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = ResNetV2Layer(32, stride=2)\n",
    "inputs = tf.random.normal((4, 32, 32, 16))\n",
    "z = layer(inputs)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNetV2Model = tf.keras.Sequential([\n",
    "    Conv2D(filters=16, kernel_size=3, padding=\"same\", use_bias=False, data_format=\"channels_last\"),\n",
    "    ResNetV2Layer(16),\n",
    "    ResNetV2Layer(16),\n",
    "    ResNetV2Layer(16),\n",
    "    ResNetV2Layer(32, stride=2),\n",
    "    ResNetV2Layer(32),\n",
    "    ResNetV2Layer(32),\n",
    "    ResNetV2Layer(64, stride=2),\n",
    "    ResNetV2Layer(64),\n",
    "    ResNetV2Layer(64),\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.random.normal((32, 32, 32, 3))\n",
    "z = ResNetV2Model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (32, 32, 32, 16)          432       \n",
      "                                                                 \n",
      " res_net_v2_layer_3 (ResNetV  (32, 32, 32, 16)         4736      \n",
      " 2Layer)                                                         \n",
      "                                                                 \n",
      " res_net_v2_layer_4 (ResNetV  (32, 32, 32, 16)         4736      \n",
      " 2Layer)                                                         \n",
      "                                                                 \n",
      " res_net_v2_layer_5 (ResNetV  (32, 32, 32, 16)         4736      \n",
      " 2Layer)                                                         \n",
      "                                                                 \n",
      " res_net_v2_layer_6 (ResNetV  (32, 16, 16, 32)         14016     \n",
      " 2Layer)                                                         \n",
      "                                                                 \n",
      " res_net_v2_layer_7 (ResNetV  (32, 16, 16, 32)         18688     \n",
      " 2Layer)                                                         \n",
      "                                                                 \n",
      " res_net_v2_layer_8 (ResNetV  (32, 16, 16, 32)         18688     \n",
      " 2Layer)                                                         \n",
      "                                                                 \n",
      " res_net_v2_layer_9 (ResNetV  (32, 8, 8, 64)           55680     \n",
      " 2Layer)                                                         \n",
      "                                                                 \n",
      " res_net_v2_layer_10 (ResNet  (32, 8, 8, 64)           74240     \n",
      " V2Layer)                                                        \n",
      "                                                                 \n",
      " res_net_v2_layer_11 (ResNet  (32, 8, 8, 64)           74240     \n",
      " V2Layer)                                                        \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (32, 64)                 0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (32, 10)                  650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270,842\n",
      "Trainable params: 269,594\n",
      "Non-trainable params: 1,248\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ResNetV2Model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds, val_ds, test_ds = tfds.load(\n",
    "#     \"cifar10\", \n",
    "#     split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
    "#     as_supervised=True)\n",
    "\n",
    "train_ds, val_ds = tfds.load(\n",
    "    \"cifar10\",\n",
    "    split=[\"train\", \"test\"],\n",
    "    as_supervised=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = [0.229, 0.224, 0.225]\n",
    "var = [x ** 2 for x in std]\n",
    "\n",
    "augment_pipeline = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(scale=1./255),\n",
    "    tf.keras.layers.Normalization(mean=[0.485, 0.456, 0.406], variance=var),\n",
    "    tf.keras.layers.ZeroPadding2D(padding=(4, 4)),\n",
    "    tf.keras.layers.RandomFlip(mode=\"horizontal\"),\n",
    "    tf.keras.layers.RandomCrop(height=32, width=32)\n",
    "])\n",
    "\n",
    "evaluate_pipeline = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(scale=1./255),\n",
    "    tf.keras.layers.Normalization(mean=[0.485, 0.456, 0.406], variance=var),\n",
    "])\n",
    "\n",
    "augment_pipeline.compile()\n",
    "evaluate_pipeline.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/trevoryu/.venv/tf_env/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/trevoryu/.venv/tf_env/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformIntV2 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformIntV2 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformIntV2 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformIntV2 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).batch(32, drop_remainder=True).map(lambda x, y: (augment_pipeline(x, training=True), y))\n",
    "val_ds = val_ds.cache().batch(32, drop_remainder=True).map(lambda x, y: (evaluate_pipeline(x, training=False), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.SGD(\n",
    "#     learning_rate=0.01, momentum=0.9,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNetV2Model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, weight_decay=1e-4),\n",
    "    # optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "        tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 17:39:28.460750: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1562/1562 [==============================] - 124s 78ms/step - loss: 1.5050 - sparse_categorical_accuracy: 0.4596 - sparse_categorical_crossentropy: 1.5050 - val_loss: 1.3193 - val_sparse_categorical_accuracy: 0.5525 - val_sparse_categorical_crossentropy: 1.3193\n",
      "Epoch 2/10\n",
      "1562/1562 [==============================] - 130s 83ms/step - loss: 1.0534 - sparse_categorical_accuracy: 0.6254 - sparse_categorical_crossentropy: 1.0534 - val_loss: 1.0919 - val_sparse_categorical_accuracy: 0.6086 - val_sparse_categorical_crossentropy: 1.0919\n",
      "Epoch 3/10\n",
      "1562/1562 [==============================] - 134s 86ms/step - loss: 0.8717 - sparse_categorical_accuracy: 0.6956 - sparse_categorical_crossentropy: 0.8717 - val_loss: 1.0078 - val_sparse_categorical_accuracy: 0.6634 - val_sparse_categorical_crossentropy: 1.0078\n",
      "Epoch 4/10\n",
      "1562/1562 [==============================] - 141s 90ms/step - loss: 0.7565 - sparse_categorical_accuracy: 0.7371 - sparse_categorical_crossentropy: 0.7565 - val_loss: 0.8892 - val_sparse_categorical_accuracy: 0.7025 - val_sparse_categorical_crossentropy: 0.8892\n",
      "Epoch 5/10\n",
      "1562/1562 [==============================] - 140s 90ms/step - loss: 0.6769 - sparse_categorical_accuracy: 0.7680 - sparse_categorical_crossentropy: 0.6769 - val_loss: 0.7833 - val_sparse_categorical_accuracy: 0.7466 - val_sparse_categorical_crossentropy: 0.7833\n",
      "Epoch 6/10\n",
      "1562/1562 [==============================] - 146s 94ms/step - loss: 0.6127 - sparse_categorical_accuracy: 0.7889 - sparse_categorical_crossentropy: 0.6127 - val_loss: 0.7087 - val_sparse_categorical_accuracy: 0.7723 - val_sparse_categorical_crossentropy: 0.7087\n",
      "Epoch 7/10\n",
      "1562/1562 [==============================] - 143s 92ms/step - loss: 0.5664 - sparse_categorical_accuracy: 0.8056 - sparse_categorical_crossentropy: 0.5664 - val_loss: 0.7203 - val_sparse_categorical_accuracy: 0.7550 - val_sparse_categorical_crossentropy: 0.7203\n",
      "Epoch 8/10\n",
      "1562/1562 [==============================] - 151s 97ms/step - loss: 0.5214 - sparse_categorical_accuracy: 0.8196 - sparse_categorical_crossentropy: 0.5214 - val_loss: 0.7719 - val_sparse_categorical_accuracy: 0.7519 - val_sparse_categorical_crossentropy: 0.7719\n",
      "Epoch 9/10\n",
      "1562/1562 [==============================] - 145s 93ms/step - loss: 0.4858 - sparse_categorical_accuracy: 0.8326 - sparse_categorical_crossentropy: 0.4858 - val_loss: 0.8339 - val_sparse_categorical_accuracy: 0.7545 - val_sparse_categorical_crossentropy: 0.8339\n",
      "Epoch 10/10\n",
      "1562/1562 [==============================] - 149s 96ms/step - loss: 0.4578 - sparse_categorical_accuracy: 0.8404 - sparse_categorical_crossentropy: 0.4578 - val_loss: 0.8480 - val_sparse_categorical_accuracy: 0.7496 - val_sparse_categorical_crossentropy: 0.8480\n"
     ]
    }
   ],
   "source": [
    "history = ResNetV2Model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 epochs SGD 0.01, 0.9: val acc 73.8, val loss 0.7473, train acc 75.5, train loss 0.7066\n",
    "\n",
    "10 epochs adamw 1e-3, 1e-4: val acc 74.9, val loss 0.848, train acc 84.0, train_loss 0.4858"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.505010724067688,\n",
       "  1.053391933441162,\n",
       "  0.8717251420021057,\n",
       "  0.7565324902534485,\n",
       "  0.676871120929718,\n",
       "  0.6126886010169983,\n",
       "  0.5663599967956543,\n",
       "  0.5213607549667358,\n",
       "  0.48580610752105713,\n",
       "  0.457795649766922],\n",
       " 'sparse_categorical_accuracy': [0.4596470892429352,\n",
       "  0.6254401206970215,\n",
       "  0.6955626010894775,\n",
       "  0.7371158599853516,\n",
       "  0.7679857611656189,\n",
       "  0.7888524532318115,\n",
       "  0.8055977821350098,\n",
       "  0.8196423053741455,\n",
       "  0.832626461982727,\n",
       "  0.8404489159584045],\n",
       " 'sparse_categorical_crossentropy': [1.505010724067688,\n",
       "  1.053391933441162,\n",
       "  0.8717251420021057,\n",
       "  0.7565324902534485,\n",
       "  0.676871120929718,\n",
       "  0.6126886010169983,\n",
       "  0.5663599967956543,\n",
       "  0.5213607549667358,\n",
       "  0.48580610752105713,\n",
       "  0.457795649766922],\n",
       " 'val_loss': [1.319312334060669,\n",
       "  1.0919350385665894,\n",
       "  1.0077588558197021,\n",
       "  0.8891614079475403,\n",
       "  0.7832885980606079,\n",
       "  0.7087368965148926,\n",
       "  0.7203460335731506,\n",
       "  0.7719316482543945,\n",
       "  0.8339065313339233,\n",
       "  0.8480474948883057],\n",
       " 'val_sparse_categorical_accuracy': [0.5524839758872986,\n",
       "  0.6085737347602844,\n",
       "  0.6633613705635071,\n",
       "  0.7025240659713745,\n",
       "  0.7465945482254028,\n",
       "  0.7723357081413269,\n",
       "  0.7550080418586731,\n",
       "  0.7519030570983887,\n",
       "  0.7545071840286255,\n",
       "  0.7495993375778198],\n",
       " 'val_sparse_categorical_crossentropy': [1.319312334060669,\n",
       "  1.0919350385665894,\n",
       "  1.0077588558197021,\n",
       "  0.8891614079475403,\n",
       "  0.7832885980606079,\n",
       "  0.7087368965148926,\n",
       "  0.7203460335731506,\n",
       "  0.7719316482543945,\n",
       "  0.8339065313339233,\n",
       "  0.8480474948883057]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 15 2022, 17:11:09) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ac8ea7b7b4726b3eeb8770dee74792e96cbee2f4a56a29410f0e6faae8b3138"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
