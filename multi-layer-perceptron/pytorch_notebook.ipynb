{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYjVAR88R2QP",
        "outputId": "7ce43d3d-be8f-414b-d847-dd1e97d10d9b"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.transforms import ToTensor\n",
        "import torch \n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "import json\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    nn.Flatten(start_dim=0),\n",
        "])\n",
        "\n",
        "train_ds= torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_ds= torchvision.datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "train_dl = torch.utils.data.DataLoader(dataset=train_ds, batch_size=128, shuffle=True)\n",
        "test_dl = torch.utils.data.DataLoader(dataset=test_ds, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiLayerPerceptron(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(784, 100)\n",
        "    self.fc2 = nn.Linear(100, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = torch.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_dl, epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    tic_train_epoch = time.time()\n",
        "\n",
        "    for data, target in train_dl:\n",
        "    \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data.to(device))\n",
        "        loss = criterion(output, target.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "        train_loss += loss.item()\n",
        "            \n",
        "    train_loss = train_loss / len(train_dl)\n",
        "    toc_train_epoch = time.time()\n",
        "    time_per_epoch = toc_train_epoch - tic_train_epoch\n",
        "    print('\\nEpoch: {} \\tTraining Loss: {:.6f} \\tEpoch Time: {:.2f}s'.format(\n",
        "    epoch+1, \n",
        "    train_loss,\n",
        "    time_per_epoch,\n",
        "    ))\n",
        "    metrics = {\n",
        "        \"epoch_train_time\": time_per_epoch,\n",
        "        \"epoch_train_loss\": train_loss\n",
        "    }\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_epoch(model, test_dl, epoch):\n",
        "    model.eval()\n",
        "    tic_eval = time.time()\n",
        "\n",
        "    test_loss = 0    \n",
        "    preds = []\n",
        "    labels = []\n",
        "    for data, target in test_dl:\n",
        "        with torch.no_grad():\n",
        "            output = model(data.to(device))\n",
        "            loss = criterion(output, target.to(device))\n",
        "\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            pred = torch.argmax(output, dim=-1).tolist()\n",
        "            preds.extend(pred)\n",
        "            labels.extend(target.tolist())\n",
        "    toc_eval = time.time()\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    print(f'Epoch: {epoch + 1 } \\t Test Loss: {test_loss / len(test_dl):.6f} \\tEpoch Time: {toc_eval - tic_eval:.2f}s')\n",
        "\n",
        "    num_eval_batches = len(test_dl)\n",
        "\n",
        "    average_batch_inference_time = (1000 * (toc_eval - tic_eval) / num_eval_batches) # in ms\n",
        "\n",
        "    print(\"Average Batch Inference Time: {:.2f}ms\".format(average_batch_inference_time))\n",
        "    metrics = {\n",
        "        \"eval_accuracy\": acc,\n",
        "        \"average_batch_inference_time\": average_batch_inference_time\n",
        "    }\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = MultiLayerPerceptron().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001, betas=[0.9, 0.999])\n",
        "\n",
        "n_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1 \tTraining Loss: 0.437623 \tEpoch Time: 10.23s\n",
            "Epoch: 1 \t Test Loss: 0.238174 \tEpoch Time: 1.53s\n",
            "Average Batch Inference Time: 19.35ms\n",
            "\n",
            "Epoch: 2 \tTraining Loss: 0.209079 \tEpoch Time: 10.13s\n",
            "Epoch: 2 \t Test Loss: 0.173932 \tEpoch Time: 1.54s\n",
            "Average Batch Inference Time: 19.55ms\n",
            "\n",
            "Epoch: 3 \tTraining Loss: 0.153987 \tEpoch Time: 10.28s\n",
            "Epoch: 3 \t Test Loss: 0.134340 \tEpoch Time: 1.55s\n",
            "Average Batch Inference Time: 19.60ms\n",
            "\n",
            "Epoch: 4 \tTraining Loss: 0.120993 \tEpoch Time: 10.25s\n",
            "Epoch: 4 \t Test Loss: 0.116795 \tEpoch Time: 1.53s\n",
            "Average Batch Inference Time: 19.41ms\n",
            "\n",
            "Epoch: 5 \tTraining Loss: 0.097512 \tEpoch Time: 10.29s\n",
            "Epoch: 5 \t Test Loss: 0.105389 \tEpoch Time: 1.55s\n",
            "Average Batch Inference Time: 19.58ms\n",
            "\n",
            "Epoch: 6 \tTraining Loss: 0.081829 \tEpoch Time: 10.28s\n",
            "Epoch: 6 \t Test Loss: 0.101438 \tEpoch Time: 1.55s\n",
            "Average Batch Inference Time: 19.60ms\n",
            "\n",
            "Epoch: 7 \tTraining Loss: 0.068962 \tEpoch Time: 10.42s\n",
            "Epoch: 7 \t Test Loss: 0.087473 \tEpoch Time: 1.53s\n",
            "Average Batch Inference Time: 19.42ms\n",
            "\n",
            "Epoch: 8 \tTraining Loss: 0.059603 \tEpoch Time: 10.29s\n",
            "Epoch: 8 \t Test Loss: 0.082074 \tEpoch Time: 1.55s\n",
            "Average Batch Inference Time: 19.67ms\n",
            "\n",
            "Epoch: 9 \tTraining Loss: 0.050895 \tEpoch Time: 10.16s\n",
            "Epoch: 9 \t Test Loss: 0.079633 \tEpoch Time: 1.52s\n",
            "Average Batch Inference Time: 19.25ms\n",
            "\n",
            "Epoch: 10 \tTraining Loss: 0.045783 \tEpoch Time: 10.14s\n",
            "Epoch: 10 \t Test Loss: 0.075055 \tEpoch Time: 1.52s\n",
            "Average Batch Inference Time: 19.22ms\n",
            "\n",
            "Total Training Time: 117.90s\n",
            "Average epoch train time: 10.25 s\n"
          ]
        }
      ],
      "source": [
        "tic_total_train = time.time()\n",
        "\n",
        "train_times = []\n",
        "for epoch in range(10):\n",
        "    train_metrics = train_epoch(model, train_dl, epoch)\n",
        "    eval_metrics = eval_epoch(model, test_dl, epoch)\n",
        "    train_times.append(train_metrics[\"epoch_train_time\"])\n",
        "toc_total_train = time.time()\n",
        "total_train_time = toc_total_train - tic_total_train\n",
        "\n",
        "print(\"\\nTotal Training Time: {:.2f}s\".format(total_train_time))\n",
        "print(f\"Average epoch train time: {np.mean(train_times):.2f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "hzKLVr20I94R"
      },
      "outputs": [],
      "source": [
        "# export JSON\n",
        "metrics = {\n",
        "    \"model_name\": \"MLP\",\n",
        "    \"framework_name\": \"PyTorch\",\n",
        "    \"dataset\": \"MNIST Digits\",\n",
        "    \"task\": \"classification\",\n",
        "    \"total_training_time\": total_train_time,         # in seconds\n",
        "    \"average_epoch_training_time\": np.mean(train_times),  # in seconds\n",
        "    \"average_batch_inference_time\": eval_metrics[\"average_batch_inference_time\"],  # in milliseconds\n",
        "    \"final_training_loss\": train_metrics[\"epoch_train_loss\"],\n",
        "    \"final_evaluation_accuracy\": eval_metrics[\"eval_accuracy\"],\n",
        "}\n",
        "\n",
        "with open(\"m1-pytorch-mlp.json\", \"w\") as outfile:\n",
        "    json.dump(metrics, outfile)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2y3I7-aRUiu",
        "outputId": "13d592dd-6343-4558-bc82-b8b8317a3f2e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'model_name': 'MLP',\n",
              " 'framework_name': 'PyTorch',\n",
              " 'dataset': 'MNIST Digits',\n",
              " 'task': 'classification',\n",
              " 'total_training_time': 117.8952624797821,\n",
              " 'average_epoch_training_time': 10.246275472640992,\n",
              " 'average_batch_inference_time': 19.221951689901232,\n",
              " 'final_training_loss': 0.04578290575309031,\n",
              " 'final_evaluation_accuracy': 0.9781}"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
