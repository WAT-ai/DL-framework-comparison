{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "717962b8",
   "metadata": {},
   "source": [
    "# Milestone 1: Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f8b80",
   "metadata": {},
   "source": [
    "Developing a simple MLP model to classify the MNIST digits dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc567535",
   "metadata": {},
   "source": [
    "## Model Specfications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce7939",
   "metadata": {},
   "source": [
    "Model: Multi-layer Perceptron (MLP)\n",
    "- Input size: 784 (28 x 28 flattened)\n",
    "- Hidden layer size: 100\n",
    "- Hidden activation function: ReLU\n",
    "- Number of outputs: 10\n",
    "- Loss function: cross entropy\n",
    "- Metric: accuracy\n",
    "\n",
    "Data: MNIST handwritten digits \n",
    "- Train/Test split: Use the MNIST split (60000,10000)\n",
    "- Pre-processing: normalize by dividing by 255, flatten from (28 x 28 x 60000) to (784 x 60000)\n",
    "- Pre-processing targets: one hot vectors\n",
    "\n",
    "Hyperparameters:\n",
    "- Optimizer: Adam\n",
    "- learning rate: 1e-4\n",
    "- beta_1: 0.9\n",
    "- beta_2: 0.999\n",
    "- Number of epochs for training: 10\n",
    "- Batch size: 128\n",
    "\n",
    "Metrics to record:\n",
    "- Total training time (from start of training script to end of training run)\n",
    "- Training time per 1 epoch (measure from start to end of each epoch and average over all epochs)\n",
    "- Inference time per batch (measure per batch and average over all batches)\n",
    "- Final training loss\n",
    "- Final evaluation accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b51afea",
   "metadata": {},
   "source": [
    "#### Importing different libraries needed for model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeda984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # libraries for dataset import\n",
    "\n",
    "# libraries needed\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet import autograd as ag\n",
    "\n",
    "# import matplotlib as plt\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# json library neded to export metrics \n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "740d46de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 3., 3.],\n",
       "       [3., 3., 3.]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick validation whether mxnet import worked\n",
    "a = mx.nd.ones((2,3))\n",
    "b = a*2 +1\n",
    "b.asnumpy()\n",
    "\n",
    "# Output should be:\n",
    "# array([[3., 3., 3.],\n",
    "#        [3., 3., 3.]], dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e1517",
   "metadata": {},
   "source": [
    "<h4> Loading and Pre-processing MNIST dataset through keras import </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f830d6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-22 11:01:00.717370: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-22 11:01:00.918270: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-10-22 11:01:00.923333: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-22 11:01:00.923348: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-22 11:01:00.951760: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-22 11:01:01.802951: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-22 11:01:01.803069: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-22 11:01:01.803076: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c45dc841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import 60000 (training) and 10000 (testing images from mnist data set\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e44c72a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying the shape of the data and the label\n",
    "# data shape is 28 x 28,\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train[128])\n",
    "\n",
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c0c6f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mx.nd.array(X_train)\n",
    "X_test = mx.nd.array(X_test)\n",
    "\n",
    "y_train = mx.nd.array(y_train)\n",
    "y_test = mx.nd.array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0ed3430",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255 \n",
    "X_test = X_test/255\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2aef3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting labels to one-hot vectors\n",
    "\n",
    "y_train = mx.nd.one_hot(y_train, 10, 1, 0)\n",
    "y_test = mx.nd.one_hot(y_test, 10, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff02f090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "<NDArray 10 @cpu(0)>\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Verifying the shape and value of one example\n",
    "print(y_train[128])\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d3b4902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a batch data iterator, with batch_size = 128\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_data = mx.io.NDArrayIter(X_train, y_train , batch_size, shuffle=True)\n",
    "val_data = mx.io.NDArrayIter(X_test, y_test, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0dc2de",
   "metadata": {},
   "source": [
    "<h5> Developing the MLP model </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69f436f2",
   "metadata": {},
   "outputs": [],
   "source": [
    " # setting up a sequential neural network initializers, layers\n",
    "net = gluon.nn.Sequential()\n",
    "    # creating a chain of neural network layers\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(100, activation = 'relu'))\n",
    "    net.add(gluon.nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0ea55f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating two contexts describing device type, ID on whihc the computation is carried on\n",
    "gpus = mx.test_utils.list_gpus()\n",
    "ctx =  [mx.gpu()] if gpus else [mx.cpu(0), mx.cpu(1)]\n",
    "# Initializing weight parameters using Xavier initliazer (could also mx.init.Zero())\n",
    "net.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n",
    "\n",
    "# Applying the Adam optimizer with its parameters according to our constraints\n",
    "trainer= gluon.Trainer(net.collect_params(), 'adam', optimizer_params = {'learning_rate': 0.0004, 'beta1': 0.9, 'beta2': 0.999})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "513ff5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc at epoch 0: accuracy=0.394106\n",
      "training acc at epoch 1: accuracy=0.238816\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m         outputs\u001b[39m.\u001b[39mappend(z)\n\u001b[1;32m     40\u001b[0m \u001b[39m# Updates internal evaluation\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m# label = mx.nd.argmax(label)\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m metric\u001b[39m.\u001b[39;49mupdate(label, outputs)\n\u001b[1;32m     43\u001b[0m \u001b[39m# Make one step of parameter update. Trainer needs to know the\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m# batch size of data to normalize the gradient by 1/batch_size.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m trainer\u001b[39m.\u001b[39mstep(batch\u001b[39m.\u001b[39mdata[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mxnet/metric.py:493\u001b[0m, in \u001b[0;36mAccuracy.update\u001b[0;34m(self, labels, preds)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39mif\u001b[39;00m pred_label\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m label\u001b[39m.\u001b[39mshape:\n\u001b[1;32m    492\u001b[0m     pred_label \u001b[39m=\u001b[39m ndarray\u001b[39m.\u001b[39margmax(pred_label, axis\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis)\n\u001b[0;32m--> 493\u001b[0m pred_label \u001b[39m=\u001b[39m pred_label\u001b[39m.\u001b[39;49masnumpy()\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mint32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    494\u001b[0m label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39masnumpy()\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mint32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    495\u001b[0m \u001b[39m# flatten before checking shapes to avoid shape miss match\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mxnet/ndarray/ndarray.py:2568\u001b[0m, in \u001b[0;36mNDArray.asnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2551\u001b[0m \u001b[39m\"\"\"Returns a ``numpy.ndarray`` object with value copied from this array.\u001b[39;00m\n\u001b[1;32m   2552\u001b[0m \n\u001b[1;32m   2553\u001b[0m \u001b[39mExamples\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2565\u001b[0m \u001b[39m       [1, 1, 1]], dtype=int32)\u001b[39;00m\n\u001b[1;32m   2566\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2567\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m-> 2568\u001b[0m check_call(_LIB\u001b[39m.\u001b[39;49mMXNDArraySyncCopyToCPU(\n\u001b[1;32m   2569\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   2570\u001b[0m     data\u001b[39m.\u001b[39;49mctypes\u001b[39m.\u001b[39;49mdata_as(ctypes\u001b[39m.\u001b[39;49mc_void_p),\n\u001b[1;32m   2571\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_size_t(data\u001b[39m.\u001b[39;49msize)))\n\u001b[1;32m   2572\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# NOTE: This is a code that I found online while running into the issue. I am trying to replicate how this works\n",
    "# but I still cannot bypass the issue that is being caused by the one-hot vector. \n",
    "\n",
    "# Adding \"sparse_label = False\" in softmax_cross_entropy_loss results in a decreasing accuracy with every epoch\n",
    "# and not including it results in the error that the shape provided is [64,10], when it should be [64,1]\n",
    "\n",
    "\n",
    "# %%time\n",
    "epoch = 10\n",
    "# Use Accuracy as the evaluation metric.\n",
    "metric = mx.metric.Accuracy()\n",
    "softmax_cross_entropy_loss = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False, batch_axis=0)\n",
    "for i in range(epoch):\n",
    "    # Reset the train data iterator.\n",
    "    train_data.reset()\n",
    "    # Loop over the train data iterator.\n",
    "    \n",
    "    for batch in train_data:\n",
    "        # Splits train data into multiple slices along batch_axis\n",
    "        # and copy each slice into a context.\n",
    "        data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n",
    "        # Splits train labels into multiple slices along batch_axis\n",
    "        # and copy each slice into a context.\n",
    "        label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = []\n",
    "        # Inside training scope\n",
    "        with ag.record():\n",
    "            for x, y in zip(data, label):\n",
    "                # y_check = [mx.nd.argmax(i) for i in y]\n",
    "            \n",
    "                z = net(x)\n",
    "                t = [mx.nd.argmax(i) for i in z]\n",
    "                # Computes softmax cross entropy loss.\n",
    "                \n",
    "                loss = softmax_cross_entropy_loss(z, y)\n",
    "                # Backpropagate the error for one iteration.\n",
    "                loss.backward()\n",
    "                outputs.append(z)\n",
    "        \n",
    "        # Updates internal evaluation\n",
    "        # label = mx.nd.argmax(label)\n",
    "        metric.update(label, outputs)\n",
    "        # Make one step of parameter update. Trainer needs to know the\n",
    "        # batch size of data to normalize the gradient by 1/batch_size.\n",
    "        trainer.step(batch.data[0].shape[0])\n",
    "    # Gets the evaluation result.\n",
    "    name, acc = metric.get()\n",
    "    # Reset evaluation result to initial state.\n",
    "    metric.reset()\n",
    "    print('training acc at epoch %d: %s=%f'%(i, name, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9415f5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "(64, 10)\n",
      "2\n",
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(z.shape)\n",
    "print(len(outputs))\n",
    "print(loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d8d8e23",
   "metadata": {},
   "outputs": [
    {
     "ename": "MXNetError",
     "evalue": "Traceback (most recent call last):\n  File \"../src/ndarray/ndarray.cc\", line 250\nNDArray.Reshape: Check failed: shape_.Size() >= shape.Size() (64 vs. 640) : target shape size is larger than the current shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m z \u001b[39m=\u001b[39m net(x)\n\u001b[1;32m     29\u001b[0m \u001b[39m# true_inds = np.argmax(y, axis = 1)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m# Computes softmax cross entropy loss.\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m loss \u001b[39m=\u001b[39m softmax_cross_entropy_loss(z, y)\n\u001b[1;32m     32\u001b[0m \u001b[39m# Backpropagate the error for one iteration.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mxnet/gluon/block.py:825\u001b[0m, in \u001b[0;36mBlock.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    823\u001b[0m     hook(\u001b[39mself\u001b[39m, args)\n\u001b[0;32m--> 825\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    827\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    828\u001b[0m     hook(\u001b[39mself\u001b[39m, args, out)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mxnet/gluon/block.py:1502\u001b[0m, in \u001b[0;36mHybridBlock.forward\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 v\u001b[39m.\u001b[39m_finish_deferred_init()\n\u001b[1;32m   1500\u001b[0m             params \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mdata(ctx) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reg_params\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m-> 1502\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhybrid_forward(ndarray, x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m   1503\u001b[0m params \u001b[39m=\u001b[39m {i: j\u001b[39m.\u001b[39mvar() \u001b[39mfor\u001b[39;00m i, j \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reg_params\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m   1504\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname_scope():\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mxnet/gluon/loss.py:392\u001b[0m, in \u001b[0;36mSoftmaxCrossEntropyLoss.hybrid_forward\u001b[0;34m(self, F, pred, label, sample_weight)\u001b[0m\n\u001b[1;32m    390\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mpick(pred, label, axis\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_axis, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    391\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 392\u001b[0m     label \u001b[39m=\u001b[39m _reshape_like(F, label, pred)\n\u001b[1;32m    393\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m(pred \u001b[39m*\u001b[39m label)\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_axis, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    394\u001b[0m loss \u001b[39m=\u001b[39m _apply_weighting(F, loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_weight, sample_weight)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mxnet/gluon/loss.py:71\u001b[0m, in \u001b[0;36m_reshape_like\u001b[0;34m(F, x, y)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m\"\"\"Reshapes x to the same shape as y.\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m F \u001b[39mis\u001b[39;00m ndarray:\n\u001b[0;32m---> 71\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39;49mreshape(y\u001b[39m.\u001b[39;49mshape)\n\u001b[1;32m     72\u001b[0m \u001b[39melif\u001b[39;00m is_np_array():\n\u001b[1;32m     73\u001b[0m     F \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnpx\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mxnet/ndarray/ndarray.py:1515\u001b[0m, in \u001b[0;36mNDArray.reshape\u001b[0;34m(self, *shape, **kwargs)\u001b[0m\n\u001b[1;32m   1512\u001b[0m handle \u001b[39m=\u001b[39m NDArrayHandle()\n\u001b[1;32m   1514\u001b[0m \u001b[39m# Actual reshape\u001b[39;00m\n\u001b[0;32m-> 1515\u001b[0m check_call(_LIB\u001b[39m.\u001b[39;49mMXNDArrayReshape64(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   1516\u001b[0m                                    \u001b[39mlen\u001b[39;49m(shape),\n\u001b[1;32m   1517\u001b[0m                                    c_array(ctypes\u001b[39m.\u001b[39;49mc_int64, shape),\n\u001b[1;32m   1518\u001b[0m                                    reverse,\n\u001b[1;32m   1519\u001b[0m                                    ctypes\u001b[39m.\u001b[39;49mbyref(handle)))\n\u001b[1;32m   1520\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m(handle\u001b[39m=\u001b[39mhandle, writable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwritable)\n\u001b[1;32m   1522\u001b[0m \u001b[39m# Array size should not change\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mxnet/base.py:246\u001b[0m, in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39m\"\"\"Check the return value of C API call.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \n\u001b[1;32m    237\u001b[0m \u001b[39mThis function will raise an exception when an error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39m    return value from API calls.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 246\u001b[0m     \u001b[39mraise\u001b[39;00m get_last_ffi_error()\n",
      "\u001b[0;31mMXNetError\u001b[0m: Traceback (most recent call last):\n  File \"../src/ndarray/ndarray.cc\", line 250\nNDArray.Reshape: Check failed: shape_.Size() >= shape.Size() (64 vs. 640) : target shape size is larger than the current shape"
     ]
    }
   ],
   "source": [
    "# NOTE: This is the code that I want to run, I just need to figure out what the issue is.\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "epoch = 10\n",
    "num_examples = X_train.shape[0]\n",
    "# Use Accuracy as the evaluation metric.\n",
    "metric = mx.metric.Accuracy()\n",
    "softmax_ce = loss.SoftmaxCrossEntropyLoss(sparse_label= False)\n",
    "\n",
    "\n",
    "for i in range(epoch):\n",
    "    cumulative_loss = 0\n",
    "    # Reset the train data iterator.\n",
    "    train_data.reset()\n",
    "    # Loop over the train data iterator.\n",
    "    for batch in train_data:\n",
    "        # Splits train data into multiple slices along batch_axis\n",
    "        # and copy each slice into separate contexts \n",
    "        data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n",
    "        # Splits labels similarly\n",
    "        label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = []\n",
    "        # Inside training scope\n",
    "        with ag.record():\n",
    "            for x, y in zip(data, label):\n",
    "                z = net(x)\n",
    "                # true_inds = np.argmax(y, axis = 1)\n",
    "                # Computes softmax cross entropy loss.\n",
    "                loss = softmax_ce(z, y)\n",
    "                # Backpropagate the error for one iteration.\n",
    "                loss.backward()\n",
    "                outputs.append(z)\n",
    "                cumulative_loss += nd.sum(loss).asscalar()\n",
    "\n",
    "        # Updates internal evaluation\n",
    "        metric.update(label, outputs)\n",
    "        # Make one step of parameter update. Trainer needs to know the\n",
    "        # batch size of data to normalize the gradient by 1/batch_size.\n",
    "        trainer.step(batch.data[0].shape[0])\n",
    "    # Gets the evaluation result.\n",
    "    name, acc = metric.get()\n",
    "    # Reset evaluation result to initial state.\n",
    "    metric.reset()    \n",
    "    t_a = []\n",
    "    tr_a = []\n",
    "    tick = []\n",
    "    # test_accuracy = evaluate_accuracy(val_data, net)\n",
    "    # t_a.append(test_accuracy)\n",
    "    # train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    # tr_a.append(train_accuracy)\n",
    "    tick.append(time.time()-tic)\n",
    "\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s,  , in %.1f sec\" %\n",
    "          (i, cumulative_loss/num_examples, acc, time.time()-tic))\n",
    "    # print('training acc at epoch %d: %s=%f'%(i, name, acc))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "856105b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a54656f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export JSON file \n",
    "\n",
    "metrics = {\n",
    "    'model_name': 'MLP',\n",
    "    'framework_name': 'MxNet',\n",
    "    'dataset': 'MNIST Digits',\n",
    "    'task': 'classification',\n",
    "    'total_training_time': tic, # fix this  \n",
    "    'average_epoch_training_time': np.average(tick), # fix this\n",
    "    'average_batch_inference_time': 25,  # fix this\n",
    "    'final_training_loss': cumulative_loss/num_examples, # fix this\n",
    "    'final_evaluation_accuracy': t_a[-1] # fix this\n",
    "}\n",
    "\n",
    "# with open('m1-mxnet-mlp.json', 'w') as outfile:\n",
    "#     json.dump(metrics, outfile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
