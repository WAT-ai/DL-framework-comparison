{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "717962b8",
   "metadata": {},
   "source": [
    "# Milestone 1: Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f8b80",
   "metadata": {},
   "source": [
    "Developing a simple MLP model to classify the MNIST digits dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc567535",
   "metadata": {},
   "source": [
    "## Model Specfications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce7939",
   "metadata": {},
   "source": [
    "Model: Multi-layer Perceptron (MLP)\n",
    "- Input size: 784 (28 x 28 flattened)\n",
    "- Hidden layer size: 100\n",
    "- Hidden activation function: ReLU\n",
    "- Number of outputs: 10\n",
    "- Loss function: cross entropy\n",
    "- Metric: accuracy\n",
    "\n",
    "Data: MNIST handwritten digits \n",
    "- Train/Test split: Use the MNIST split (60000,10000)\n",
    "- Pre-processing: normalize by dividing by 255, flatten from (28 x 28 x 60000) to (784 x 60000)\n",
    "- Pre-processing targets: one hot vectors\n",
    "\n",
    "Hyperparameters:\n",
    "- Optimizer: Adam\n",
    "- learning rate: 1e-4\n",
    "- beta_1: 0.9\n",
    "- beta_2: 0.999\n",
    "- Number of epochs for training: 10\n",
    "- Batch size: 128\n",
    "\n",
    "Metrics to record:\n",
    "- Total training time (from start of training script to end of training run)\n",
    "- Training time per 1 epoch (measure from start to end of each epoch and average over all epochs)\n",
    "- Inference time per batch (measure per batch and average over all batches)\n",
    "- Final training loss\n",
    "- Final evaluation accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b51afea",
   "metadata": {},
   "source": [
    "#### Importing different libraries needed for model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeda984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # libraries for dataset import\n",
    "\n",
    "# libraries needed\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd as ag, nd\n",
    "\n",
    "# import matplotlib as plt\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# json library neded to export metrics \n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "740d46de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 3., 3.],\n",
       "       [3., 3., 3.]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick validation whether mxnet import worked\n",
    "# NOTE: this won't be needed for the python script, this is just a double check\n",
    "\n",
    "a = mx.nd.ones((2,3))\n",
    "b = a*2 +1\n",
    "b.asnumpy()\n",
    "\n",
    "# Output should be:\n",
    "# array([[3., 3., 3.],\n",
    "#        [3., 3., 3.]], dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e1517",
   "metadata": {},
   "source": [
    "<h4> Loading and Pre-processing MNIST dataset through keras import </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f830d6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-23 14:33:54.151303: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-23 14:33:54.315485: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-10-23 14:33:54.320462: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-23 14:33:54.320483: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-23 14:33:54.344555: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-23 14:33:55.067571: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-23 14:33:55.067672: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-23 14:33:55.067680: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c45dc841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import 60000 (training) and 10000 (testing images from mnist data set\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e44c72a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying the shape of the data and the label\n",
    "# data shape is 28 x 28,\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train[128])\n",
    "\n",
    "type(X_train) # data type is np.ndarray. Better to change to mx.nd.array to avoid any issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c0c6f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the np.array to mx.nd.array\n",
    "\n",
    "X_train = mx.nd.array(X_train)\n",
    "X_test = mx.nd.array(X_test)\n",
    "\n",
    "y_train = mx.nd.array(y_train)\n",
    "y_test = mx.nd.array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0ed3430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the training values + reshaping\n",
    "\n",
    "X_train = X_train/255 \n",
    "X_test = X_test/255\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2aef3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting y-labels to one-hot vectors\n",
    "\n",
    "y_train = mx.nd.one_hot(y_train, 10)\n",
    "y_test = mx.nd.one_hot(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff02f090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "<NDArray 10 @cpu(0)>\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Verifying the shape and value of one example\n",
    "\n",
    "print(y_train[128])\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d3b4902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a batch data iterator, with batch_size = 128\n",
    "batch_size = 128\n",
    "\n",
    "train_data = mx.io.NDArrayIter(X_train, y_train , batch_size, shuffle=True) # shuffle = True since order doesn't particularly matter\n",
    "val_data = mx.io.NDArrayIter(X_test, y_test, batch_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0dc2de",
   "metadata": {},
   "source": [
    "<h5> Developing the MLP model </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69f436f2",
   "metadata": {},
   "outputs": [],
   "source": [
    " # setting up a sequential neural network initializers, layers\n",
    "net = gluon.nn.Sequential()\n",
    "    # creating a chain of neural network layers (one hidden layer, and an output layer with 10 output vars)\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(100, activation = 'relu'))\n",
    "    net.add(gluon.nn.Dense(10))\n",
    "# Initializing the parameters \n",
    "\n",
    "net.initialize()\n",
    "# Applying the Adam optimizer with its parameters according to our constraints\n",
    "\n",
    "trainer= gluon.Trainer(net.collect_params(), 'adam', optimizer_params = {'learning_rate': 0.0004, 'beta1': 0.9, 'beta2': 0.999})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03ab1b9",
   "metadata": {},
   "source": [
    "<h4> Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d8d8e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 0.675581, Train_acc: 0.841968, in 1.44s\n",
      "Epoch 1 | Loss: 0.288800, Train_acc: 0.918910, in 1.29s\n",
      "Epoch 2 | Loss: 0.236203, Train_acc: 0.933885, in 1.21s\n",
      "Epoch 3 | Loss: 0.200724, Train_acc: 0.943947, in 1.53s\n",
      "Epoch 4 | Loss: 0.174395, Train_acc: 0.950510, in 1.37s\n",
      "Epoch 5 | Loss: 0.154541, Train_acc: 0.956606, in 1.36s\n",
      "Epoch 6 | Loss: 0.137877, Train_acc: 0.960788, in 1.36s\n",
      "Epoch 7 | Loss: 0.124221, Train_acc: 0.964852, in 1.29s\n",
      "Epoch 8 | Loss: 0.112817, Train_acc: 0.968150, in 1.63s\n",
      "Epoch 9 | Loss: 0.102236, Train_acc: 0.970866, in 1.44s\n",
      "----------------------------------------------------------------------\n",
      "CPU times: user 1min, sys: 3.54 s, total: 1min 3s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Initializing time related variables and lists (to make it easier for metric outputs)\n",
    "tic = time.thread_time()\n",
    "# time list vars for epoch times\n",
    "tick = []\n",
    "timer = []\n",
    "tick.append(time.thread_time()-tic)\n",
    "\n",
    "# time list vars for batch inference time\n",
    "b_tick = []\n",
    "b_timer = []\n",
    "b_tick.append(time.thread_time()-tic)\n",
    "\n",
    "epoch = 10\n",
    "num_examples = X_train.shape[0]\n",
    "\n",
    "# Use Accuracy as the evaluation metric.\n",
    "metric = mx.metric.Accuracy()\n",
    "\n",
    "# Using Softmax Cross Entropy for the loss function (make sure to set sparse_label = False)\n",
    "softmax_ce = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False)\n",
    "\n",
    "\n",
    "for i in range(epoch):\n",
    "    # creating a cumulative loss variable\n",
    "    cum_loss = 0\n",
    "    # Reset the train Data Iterator.\n",
    "    train_data.reset()\n",
    "    # Loop over the training Data Tterator.\n",
    "    for batch in train_data:\n",
    "        # Splits train data and its labels into multiple slices\n",
    "        # one slice will be used since we are just using 1 context\n",
    "        data = gluon.utils.split_data(batch.data[0], batch_axis=0, num_slice = 1)\n",
    "        label = gluon.utils.split_data(batch.label[0], batch_axis=0, num_slice = 1)\n",
    "\n",
    "        # initializing var to store the output values from the model\n",
    "        outputs = []\n",
    "\n",
    "        # Inside the training scope\n",
    "        with ag.record():\n",
    "            for x, y in zip(data, label):\n",
    "                # inputting the data into the network \n",
    "                z = net(x)\n",
    "\n",
    "                # Computing softmax cross entropy loss.\n",
    "                loss = softmax_ce(z, y)\n",
    "\n",
    "                # Backpropagate the error for one iteration.\n",
    "                loss.backward()\n",
    "                outputs.append(z)\n",
    "\n",
    "                # summation of the loss (will be divided by the sample_size at the end of the epoch)\n",
    "                cum_loss += nd.sum(loss).asscalar()\n",
    "        \n",
    "        # Decoding the 1H encoded data \n",
    "        # (this is IMPORTANT since it affects the input shape and will give an error)\n",
    "        # metric.update takes inputs of a list of ND array so it is to be as type list \n",
    "        label = [np.argmax(mx.nd.array(label[0]), axis = 1)]\n",
    "\n",
    "        # Evaluating the accuracy based on the training batch datasets\n",
    "        metric.update(label, outputs)\n",
    "        # Make one step of parameter update. Trainer needs to know the\n",
    "        # batch size of data to normalize the gradient by 1/batch_size.\n",
    "        trainer.step(batch.data[0].shape[0])\n",
    "        b_tick.append(time.thread_time()-tic)\n",
    "        b_timer.append(b_tick[-1]-b_tick[-2])\n",
    "    \n",
    "    # Gets the evaluation result.\n",
    "    name, acc = metric.get()\n",
    "    # Reset evaluation result to initial state.\n",
    "    metric.reset()    \n",
    "\n",
    "    # evaluating the time elapsed between one epoch\n",
    "    tick.append(time.thread_time()-tic)\n",
    "    timer.append(tick[-1]-tick[-2])\n",
    "\n",
    "    # resetting the accuracy metric for next epoch\n",
    "    metric.reset()\n",
    "    print(\"Epoch %s | Loss: %.6f, Train_acc: %.6f, in %.2fs\" %\n",
    "    (i, cum_loss/num_examples, acc, timer[i]))\n",
    "print(\"-\"*70)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52479955",
   "metadata": {},
   "source": [
    "<h4> Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c2d9135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reseting the validation Data Iterator\n",
    "val_data.reset()\n",
    "\n",
    "# Loop over the validation Data Iterator.\n",
    "for batch in val_data:\n",
    "    # Splits val data and its labels into multiple slices\n",
    "    data = gluon.utils.split_data(batch.data[0], batch_axis=0, num_slice = 1)\n",
    "    label = gluon.utils.split_data(batch.label[0], batch_axis=0, num_slice = 1)\n",
    "\n",
    "    # Initializing the model output var\n",
    "    outputs = []\n",
    "    for x in data:\n",
    "        outputs.append(net(x))\n",
    "\n",
    "    # Evaluating the accuracy of the model based on val batch datasets\n",
    "    label = [np.argmax(mx.nd.array(label[0]), axis = 1)]\n",
    "    metric.update(label, outputs)\n",
    "\n",
    "    # metric.get ouputs as (label, value), so will use val_acc[1]\n",
    "    val_acc = metric.get()\n",
    "# assert metric.get()[1] > 0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a54656f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export JSON file \n",
    "metrics = {\n",
    "    'model_name': 'MLP',\n",
    "    'framework_name': 'MxNet',\n",
    "    'dataset': 'MNIST Digits',\n",
    "    'task': 'classification',\n",
    "    'total_training_time': np.sum(timer), \n",
    "    'average_epoch_training_time': np.average(timer), \n",
    "    'average_batch_inference_time': np.average(b_timer),\n",
    "    'final_training_loss': np.float64(mx.nd.mean(loss).asnumpy()[0]), \n",
    "    'final_evaluation_accuracy': val_acc[1] \n",
    "}\n",
    "\n",
    "with open('m1-mxnet-mlp.json', 'w') as outfile:\n",
    "    json.dump(metrics, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e9e657a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'MLP',\n",
       " 'framework_name': 'MxNet',\n",
       " 'dataset': 'MNIST Digits',\n",
       " 'task': 'classification',\n",
       " 'total_training_time': 13.9214737,\n",
       " 'average_epoch_training_time': 1.39214737,\n",
       " 'average_batch_inference_time': 0.0029683269509594885,\n",
       " 'final_training_loss': 0.07210715860128403,\n",
       " 'final_evaluation_accuracy': 0.9684533227848101}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
